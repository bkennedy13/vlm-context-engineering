{
  "level": "level2_semantic_merged",
  "config": {
    "k": 10,
    "fps": 1,
    "base_chunk_size": 5,
    "merge_threshold": 0.7,
    "length_caps": {
      "short": 30,
      "medium": 30,
      "long": 60
    },
    "retrieval": "merged_chunks_clip_5frame_avg",
    "description_model": "Qwen2-VL-2B-Instruct",
    "answering_model": "Qwen2.5-VL-7B-Instruct"
  },
  "partial": false,
  "total_questions": 150,
  "correct": 89,
  "accuracy": 0.5933333333333334,
  "avg_retrieval_time": 9.338263405164083,
  "avg_inference_time": 4.45433762550354,
  "total_time": 2078.8930337429047,
  "task_type_stats": {
    "Action Reasoning": {
      "total": 14,
      "correct": 8,
      "accuracy": 0.5714285714285714
    },
    "Spatial Perception": {
      "total": 5,
      "correct": 3,
      "accuracy": 0.6
    },
    "Object Reasoning": {
      "total": 27,
      "correct": 12,
      "accuracy": 0.4444444444444444
    },
    "Temporal Reasoning": {
      "total": 7,
      "correct": 2,
      "accuracy": 0.2857142857142857
    },
    "OCR Problems": {
      "total": 4,
      "correct": 4,
      "accuracy": 1.0
    },
    "Action Recognition": {
      "total": 18,
      "correct": 10,
      "accuracy": 0.5555555555555556
    },
    "Spatial Reasoning": {
      "total": 4,
      "correct": 3,
      "accuracy": 0.75
    },
    "Object Recognition": {
      "total": 23,
      "correct": 16,
      "accuracy": 0.6956521739130435
    },
    "Temporal Perception": {
      "total": 2,
      "correct": 2,
      "accuracy": 1.0
    },
    "Information Synopsis": {
      "total": 15,
      "correct": 13,
      "accuracy": 0.8666666666666667
    },
    "Attribute Perception": {
      "total": 15,
      "correct": 9,
      "accuracy": 0.6
    },
    "Counting Problem": {
      "total": 16,
      "correct": 7,
      "accuracy": 0.4375
    }
  },
  "duration_stats": {
    "short": {
      "total": 60,
      "correct": 36,
      "accuracy": 0.6,
      "avg_retrieval_time": 1.410925555229187,
      "avg_inference_time": 4.30285016298294,
      "avg_chunk_count": 7.85,
      "avg_frames_used": 68.83333333333333,
      "avg_chunk_length": 10.105555555555556
    },
    "medium": {
      "total": 60,
      "correct": 41,
      "accuracy": 0.6833333333333333,
      "avg_retrieval_time": 6.94440647761027,
      "avg_inference_time": 4.6180558125178015,
      "avg_chunk_count": 10.0,
      "avg_frames_used": 135.41666666666666,
      "avg_chunk_length": 13.541666666666666
    },
    "long": {
      "total": 30,
      "correct": 12,
      "accuracy": 0.4,
      "avg_retrieval_time": 29.9806529601415,
      "avg_inference_time": 4.429876176516215,
      "avg_chunk_count": 10.0,
      "avg_frames_used": 214.66666666666666,
      "avg_chunk_length": 21.466666666666665
    }
  },
  "semantic_stats": {
    "avg_chunks_per_question": 9.14,
    "avg_frames_per_question": 124.63333333333334,
    "avg_chunk_length": 13.752222222222223,
    "max_chunk_length": 60,
    "avg_top_similarity": 0.304821034570535,
    "avg_mean_similarity": 0.28789528320233027
  },
  "results": [
    {
      "video_id": "058",
      "question_id": "058-1",
      "task_type": "Spatial Reasoning",
      "duration": "short",
      "question": "Based on the information provided by the video, which one is the most direct cause of the phenomenon that the smoke flows towards the lamp?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 11.680899858474731,
      "inference_time": 12.794514894485474,
      "total_chunks": 9,
      "total_frames_used": 90,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.0,
      "top_similarity": 0.2783387005329132,
      "mean_similarity": 0.2471432089805603
    },
    {
      "video_id": "058",
      "question_id": "058-2",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Which color are the mountains in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.2124254703521729,
      "inference_time": 4.815433979034424,
      "total_chunks": 9,
      "total_frames_used": 90,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.0,
      "top_similarity": 0.269824743270874,
      "mean_similarity": 0.23756790161132812
    },
    {
      "video_id": "058",
      "question_id": "058-3",
      "task_type": "Spatial Reasoning",
      "duration": "short",
      "question": "What is one of the causes of wind according to the video?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 1.2249577045440674,
      "inference_time": 3.8961715698242188,
      "total_chunks": 9,
      "total_frames_used": 90,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.0,
      "top_similarity": 0.3508243262767792,
      "mean_similarity": 0.27246904373168945
    },
    {
      "video_id": "013",
      "question_id": "013-1",
      "task_type": "Spatial Perception",
      "duration": "short",
      "question": "Which of the following is visible in the background of the video when the miniature bottle is shown empty?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 1.0831286907196045,
      "inference_time": 3.9001691341400146,
      "total_chunks": 9,
      "total_frames_used": 70,
      "max_chunk_length": 15,
      "avg_chunk_length": 7.777777777777778,
      "top_similarity": 0.32198619842529297,
      "mean_similarity": 0.3068675398826599
    },
    {
      "video_id": "013",
      "question_id": "013-2",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "According to the video, which of the following ingredients is not used in the artwork?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 1.038506031036377,
      "inference_time": 3.8892109394073486,
      "total_chunks": 9,
      "total_frames_used": 70,
      "max_chunk_length": 15,
      "avg_chunk_length": 7.777777777777778,
      "top_similarity": 0.3058723509311676,
      "mean_similarity": 0.2826395332813263
    },
    {
      "video_id": "013",
      "question_id": "013-3",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What was the next step taken by the operator after filling the bottle with oil?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 1.0226120948791504,
      "inference_time": 4.2759928703308105,
      "total_chunks": 9,
      "total_frames_used": 70,
      "max_chunk_length": 15,
      "avg_chunk_length": 7.777777777777778,
      "top_similarity": 0.29008275270462036,
      "mean_similarity": 0.2749570310115814
    },
    {
      "video_id": "141",
      "question_id": "141-1",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which player does the video call for to finally put the ball in the basket?",
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.3314063549041748,
      "inference_time": 3.297565460205078,
      "total_chunks": 2,
      "total_frames_used": 15,
      "max_chunk_length": 10,
      "avg_chunk_length": 7.5,
      "top_similarity": 0.2935510575771332,
      "mean_similarity": 0.28635525703430176
    },
    {
      "video_id": "141",
      "question_id": "141-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which player is running the pick-and-roll for the other players?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.28437280654907227,
      "inference_time": 3.2454140186309814,
      "total_chunks": 2,
      "total_frames_used": 15,
      "max_chunk_length": 10,
      "avg_chunk_length": 7.5,
      "top_similarity": 0.26493605971336365,
      "mean_similarity": 0.2603931427001953
    },
    {
      "video_id": "141",
      "question_id": "141-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which sport's tactics are shown in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.29433560371398926,
      "inference_time": 3.237290859222412,
      "total_chunks": 2,
      "total_frames_used": 15,
      "max_chunk_length": 10,
      "avg_chunk_length": 7.5,
      "top_similarity": 0.23515525460243225,
      "mean_similarity": 0.2260044515132904
    },
    {
      "video_id": "126",
      "question_id": "126-1",
      "task_type": "Spatial Reasoning",
      "duration": "short",
      "question": "What is the atmosphere portrayed in the video like?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.6893584728240967,
      "inference_time": 3.251944065093994,
      "total_chunks": 6,
      "total_frames_used": 30,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.2615668475627899,
      "mean_similarity": 0.2536208927631378
    },
    {
      "video_id": "126",
      "question_id": "126-2",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "What is the reason for entering cold water in the video during the winter season?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.5764808654785156,
      "inference_time": 3.252721071243286,
      "total_chunks": 6,
      "total_frames_used": 30,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.30173200368881226,
      "mean_similarity": 0.288994163274765
    },
    {
      "video_id": "126",
      "question_id": "126-3",
      "task_type": "OCR Problems",
      "duration": "short",
      "question": "What is the approximate temperature shown in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.5660848617553711,
      "inference_time": 3.2711145877838135,
      "total_chunks": 6,
      "total_frames_used": 30,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.31353050470352173,
      "mean_similarity": 0.28598806262016296
    },
    {
      "video_id": "115",
      "question_id": "115-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "What is the color pattern of the cat in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.6140239238739014,
      "inference_time": 4.812746286392212,
      "total_chunks": 10,
      "total_frames_used": 105,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.5,
      "top_similarity": 0.3063783347606659,
      "mean_similarity": 0.28960809111595154
    },
    {
      "video_id": "115",
      "question_id": "115-2",
      "task_type": "OCR Problems",
      "duration": "short",
      "question": "According to the video, what is the age range of the cat in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 1.5619657039642334,
      "inference_time": 4.580532789230347,
      "total_chunks": 10,
      "total_frames_used": 105,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.5,
      "top_similarity": 0.29862678050994873,
      "mean_similarity": 0.2820892930030823
    },
    {
      "video_id": "115",
      "question_id": "115-3",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many streams did the cat cross in the video?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 1.5548095703125,
      "inference_time": 4.607941627502441,
      "total_chunks": 10,
      "total_frames_used": 105,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.5,
      "top_similarity": 0.28830569982528687,
      "mean_similarity": 0.2677958607673645
    },
    {
      "video_id": "072",
      "question_id": "072-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which item does not the man wear in this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 1.3802177906036377,
      "inference_time": 4.911970376968384,
      "total_chunks": 10,
      "total_frames_used": 105,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.5,
      "top_similarity": 0.24881669878959656,
      "mean_similarity": 0.23399624228477478
    },
    {
      "video_id": "072",
      "question_id": "072-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "According to the video, what is the right pose demonstrated in the animation when sitting on the bike before starting to ride?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.3193459510803223,
      "inference_time": 5.0128114223480225,
      "total_chunks": 10,
      "total_frames_used": 105,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.5,
      "top_similarity": 0.30335408449172974,
      "mean_similarity": 0.2786497473716736
    },
    {
      "video_id": "072",
      "question_id": "072-3",
      "task_type": "Temporal Reasoning",
      "duration": "short",
      "question": "What is the sequence of steps introduced in this video?\n(a) Walk and practice using the brakes.\n(b) Glide to practice balancing.\n(c) Find a good space and safety considerations.\n(d) Start to pedal.\n(e) Adjust the seat.",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 1.3357162475585938,
      "inference_time": 5.041442632675171,
      "total_chunks": 10,
      "total_frames_used": 105,
      "max_chunk_length": 30,
      "avg_chunk_length": 10.5,
      "top_similarity": 0.3130977749824524,
      "mean_similarity": 0.27944129705429077
    },
    {
      "video_id": "053",
      "question_id": "053-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "How does the color of lava change when exposed to the air for a short while?",
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 1.4207777976989746,
      "inference_time": 4.353492736816406,
      "total_chunks": 10,
      "total_frames_used": 85,
      "max_chunk_length": 15,
      "avg_chunk_length": 8.5,
      "top_similarity": 0.37081414461135864,
      "mean_similarity": 0.34035056829452515
    },
    {
      "video_id": "053",
      "question_id": "053-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "According to the video, how do the geologists collect lava safely?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 1.347172737121582,
      "inference_time": 3.3038246631622314,
      "total_chunks": 10,
      "total_frames_used": 85,
      "max_chunk_length": 15,
      "avg_chunk_length": 8.5,
      "top_similarity": 0.39103996753692627,
      "mean_similarity": 0.36449503898620605
    },
    {
      "video_id": "053",
      "question_id": "053-3",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "How do the geologists cool the lava down?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 1.3786323070526123,
      "inference_time": 3.307445764541626,
      "total_chunks": 10,
      "total_frames_used": 85,
      "max_chunk_length": 15,
      "avg_chunk_length": 8.5,
      "top_similarity": 0.3830459415912628,
      "mean_similarity": 0.3400753438472748
    },
    {
      "video_id": "280",
      "question_id": "280-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which kind of pets is not introduced in this video?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 1.4006993770599365,
      "inference_time": 3.3266255855560303,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.33007436990737915,
      "mean_similarity": 0.30325210094451904
    },
    {
      "video_id": "280",
      "question_id": "280-2",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "How many people are in the family that is playing with a dog at the end of the video?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 1.3610610961914062,
      "inference_time": 4.85667610168457,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.32650330662727356,
      "mean_similarity": 0.2786129117012024
    },
    {
      "video_id": "280",
      "question_id": "280-3",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Which color of fish is absent from the video?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 1.3616068363189697,
      "inference_time": 2.2129204273223877,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3431803584098816,
      "mean_similarity": 0.2798433303833008
    },
    {
      "video_id": "045",
      "question_id": "045-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "What astronomical phenomenon is depicted in the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 1.5744047164916992,
      "inference_time": 3.2897331714630127,
      "total_chunks": 10,
      "total_frames_used": 55,
      "max_chunk_length": 10,
      "avg_chunk_length": 5.5,
      "top_similarity": 0.3299703598022461,
      "mean_similarity": 0.3099345862865448
    },
    {
      "video_id": "045",
      "question_id": "045-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which of the following is NOT known to occur during a total solar eclipse, according to the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 1.427520990371704,
      "inference_time": 3.9642975330352783,
      "total_chunks": 10,
      "total_frames_used": 55,
      "max_chunk_length": 10,
      "avg_chunk_length": 5.5,
      "top_similarity": 0.34044212102890015,
      "mean_similarity": 0.3160058856010437
    },
    {
      "video_id": "045",
      "question_id": "045-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Where were the people in the video located while observing the total solar eclipse?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 1.3974168300628662,
      "inference_time": 3.2987396717071533,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3429887890815735,
      "mean_similarity": 0.30354228615760803
    },
    {
      "video_id": "217",
      "question_id": "217-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "What is the physique of the athletes?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.7856636047363281,
      "inference_time": 5.00152850151062,
      "total_chunks": 3,
      "total_frames_used": 50,
      "max_chunk_length": 30,
      "avg_chunk_length": 16.666666666666668,
      "top_similarity": 0.27744972705841064,
      "mean_similarity": 0.2659565210342407
    },
    {
      "video_id": "217",
      "question_id": "217-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which sports in this video involve athletes utilizing humans as tools?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.681199312210083,
      "inference_time": 5.023286819458008,
      "total_chunks": 3,
      "total_frames_used": 50,
      "max_chunk_length": 30,
      "avg_chunk_length": 16.666666666666668,
      "top_similarity": 0.27640092372894287,
      "mean_similarity": 0.27355238795280457
    },
    {
      "video_id": "217",
      "question_id": "217-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "What do the athletes wear during their performance?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.6771972179412842,
      "inference_time": 5.019756317138672,
      "total_chunks": 3,
      "total_frames_used": 50,
      "max_chunk_length": 30,
      "avg_chunk_length": 16.666666666666668,
      "top_similarity": 0.2889130413532257,
      "mean_similarity": 0.28721293807029724
    },
    {
      "video_id": "017",
      "question_id": "017-1",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "Based on the information provided by the video, what is the maximum number of fingers required to play this piece of music?",
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 1.158735752105713,
      "inference_time": 4.882173776626587,
      "total_chunks": 6,
      "total_frames_used": 95,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.833333333333334,
      "top_similarity": 0.3205774426460266,
      "mean_similarity": 0.3085905909538269
    },
    {
      "video_id": "017",
      "question_id": "017-2",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "What do the characters in this video mean?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 1.1709516048431396,
      "inference_time": 3.4831626415252686,
      "total_chunks": 6,
      "total_frames_used": 95,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.833333333333334,
      "top_similarity": 0.2639106512069702,
      "mean_similarity": 0.24929094314575195
    },
    {
      "video_id": "017",
      "question_id": "017-3",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "In this video, why some keys are blue while others are green?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 1.1547868251800537,
      "inference_time": 5.223429918289185,
      "total_chunks": 6,
      "total_frames_used": 95,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.833333333333334,
      "top_similarity": 0.32547757029533386,
      "mean_similarity": 0.31177523732185364
    },
    {
      "video_id": "016",
      "question_id": "016-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "When was the painting mentioned in the video painted?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 1.795316219329834,
      "inference_time": 2.488255500793457,
      "total_chunks": 10,
      "total_frames_used": 50,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.35784536600112915,
      "mean_similarity": 0.33370473980903625
    },
    {
      "video_id": "016",
      "question_id": "016-2",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "Which of the following elements does not appear in the paintings primarily mentioned in the video?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 1.6555981636047363,
      "inference_time": 2.4866878986358643,
      "total_chunks": 10,
      "total_frames_used": 50,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.3260347843170166,
      "mean_similarity": 0.31164172291755676
    },
    {
      "video_id": "016",
      "question_id": "016-3",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "Which of the following elements is not present in the \"Starry Sky\"?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 1.6455798149108887,
      "inference_time": 2.5632612705230713,
      "total_chunks": 10,
      "total_frames_used": 50,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.30003464221954346,
      "mean_similarity": 0.2651163637638092
    },
    {
      "video_id": "048",
      "question_id": "048-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which galaxies are depicted in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.9269626140594482,
      "inference_time": 4.043435096740723,
      "total_chunks": 10,
      "total_frames_used": 90,
      "max_chunk_length": 15,
      "avg_chunk_length": 9.0,
      "top_similarity": 0.3308287262916565,
      "mean_similarity": 0.32183027267456055
    },
    {
      "video_id": "048",
      "question_id": "048-2",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "What significant event is the number at the bottom of the video referring to?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 1.8210837841033936,
      "inference_time": 4.786188364028931,
      "total_chunks": 10,
      "total_frames_used": 90,
      "max_chunk_length": 15,
      "avg_chunk_length": 9.0,
      "top_similarity": 0.30882197618484497,
      "mean_similarity": 0.2914171814918518
    },
    {
      "video_id": "048",
      "question_id": "048-3",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "Which of the following statements can be inferred about the Triangulum Galaxy (M33) based on the information presented in the video about the other two galaxies?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 1.8140299320220947,
      "inference_time": 4.809466123580933,
      "total_chunks": 10,
      "total_frames_used": 80,
      "max_chunk_length": 15,
      "avg_chunk_length": 8.0,
      "top_similarity": 0.3125039339065552,
      "mean_similarity": 0.305415540933609
    },
    {
      "video_id": "112",
      "question_id": "112-1",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many birds can be observed perched on the street lamp in the first half of the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 1.3820910453796387,
      "inference_time": 3.3637399673461914,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 15,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3057146668434143,
      "mean_similarity": 0.2663785219192505
    },
    {
      "video_id": "112",
      "question_id": "112-2",
      "task_type": "Information Synopsis",
      "duration": "short",
      "question": "What do the people in the village show in the video used to illuminate the night?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 1.3588438034057617,
      "inference_time": 3.3839006423950195,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 15,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3257664740085602,
      "mean_similarity": 0.29064303636550903
    },
    {
      "video_id": "112",
      "question_id": "112-3",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "What is the purpose of collecting sap from a rubber tree in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.3554880619049072,
      "inference_time": 4.29611611366272,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 15,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3351152241230011,
      "mean_similarity": 0.29348936676979065
    },
    {
      "video_id": "120",
      "question_id": "120-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "What is the animal in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.6548328399658203,
      "inference_time": 5.5535712242126465,
      "total_chunks": 2,
      "total_frames_used": 50,
      "max_chunk_length": 30,
      "avg_chunk_length": 25.0,
      "top_similarity": 0.299162894487381,
      "mean_similarity": 0.29902875423431396
    },
    {
      "video_id": "120",
      "question_id": "120-2",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "What is the reason for the lamp's action in the video, slapping the human portrayed?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.6379566192626953,
      "inference_time": 5.653092384338379,
      "total_chunks": 2,
      "total_frames_used": 50,
      "max_chunk_length": 30,
      "avg_chunk_length": 25.0,
      "top_similarity": 0.2640475034713745,
      "mean_similarity": 0.2636834383010864
    },
    {
      "video_id": "120",
      "question_id": "120-3",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Which color of clothing is the person wearing in the video?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.6332764625549316,
      "inference_time": 5.497868061065674,
      "total_chunks": 2,
      "total_frames_used": 50,
      "max_chunk_length": 30,
      "avg_chunk_length": 25.0,
      "top_similarity": 0.23111982643604279,
      "mean_similarity": 0.2290257215499878
    },
    {
      "video_id": "259",
      "question_id": "259-1",
      "task_type": "OCR Problems",
      "duration": "short",
      "question": "What time does the man in the video get up?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 1.5047481060028076,
      "inference_time": 3.3542957305908203,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.31425559520721436,
      "mean_similarity": 0.2930213510990143
    },
    {
      "video_id": "259",
      "question_id": "259-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which activity is not shown in the video?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 1.4064748287200928,
      "inference_time": 4.051955938339233,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.29228460788726807,
      "mean_similarity": 0.28064343333244324
    },
    {
      "video_id": "259",
      "question_id": "259-3",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What does the man shown in the video do after swimming?",
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 1.4086740016937256,
      "inference_time": 3.3443996906280518,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.2870407998561859,
      "mean_similarity": 0.27243220806121826
    },
    {
      "video_id": "014",
      "question_id": "014-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which of the following can be identified as Susan White-Oakes' most recent art piece, based on the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.4024057388305664,
      "inference_time": 5.839409112930298,
      "total_chunks": 6,
      "total_frames_used": 115,
      "max_chunk_length": 30,
      "avg_chunk_length": 19.166666666666668,
      "top_similarity": 0.2939489781856537,
      "mean_similarity": 0.26037582755088806
    },
    {
      "video_id": "014",
      "question_id": "014-2",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Based on the video, what is Susan White-Oakes' latest art work made of?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.3223390579223633,
      "inference_time": 5.834841728210449,
      "total_chunks": 6,
      "total_frames_used": 115,
      "max_chunk_length": 30,
      "avg_chunk_length": 19.166666666666668,
      "top_similarity": 0.30662792921066284,
      "mean_similarity": 0.26974770426750183
    },
    {
      "video_id": "014",
      "question_id": "014-3",
      "task_type": "Temporal Reasoning",
      "duration": "short",
      "question": "What does Susan White-Oakes often do before polishing the whole body?",
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 1.3294365406036377,
      "inference_time": 6.0829455852508545,
      "total_chunks": 6,
      "total_frames_used": 115,
      "max_chunk_length": 30,
      "avg_chunk_length": 19.166666666666668,
      "top_similarity": 0.3018132150173187,
      "mean_similarity": 0.2664813995361328
    },
    {
      "video_id": "288",
      "question_id": "288-1",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What were the movements of the individuals in the video during the competition process?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 1.0235188007354736,
      "inference_time": 5.8048460483551025,
      "total_chunks": 4,
      "total_frames_used": 60,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.0,
      "top_similarity": 0.2584104835987091,
      "mean_similarity": 0.2579447031021118
    },
    {
      "video_id": "288",
      "question_id": "288-2",
      "task_type": "Spatial Perception",
      "duration": "short",
      "question": "Where does the participant need to place his/her hand during the game in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.8620591163635254,
      "inference_time": 5.872333765029907,
      "total_chunks": 4,
      "total_frames_used": 60,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.0,
      "top_similarity": 0.26848578453063965,
      "mean_similarity": 0.2611212730407715
    },
    {
      "video_id": "288",
      "question_id": "288-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Who is the last to reach the finish line at the end of the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.872807502746582,
      "inference_time": 5.930265665054321,
      "total_chunks": 4,
      "total_frames_used": 60,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.0,
      "top_similarity": 0.2610059082508087,
      "mean_similarity": 0.25933516025543213
    },
    {
      "video_id": "215",
      "question_id": "215-1",
      "task_type": "Information Synopsis",
      "duration": "short",
      "question": "What is this video mainly about?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 1.8372819423675537,
      "inference_time": 3.4529881477355957,
      "total_chunks": 10,
      "total_frames_used": 65,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.5,
      "top_similarity": 0.270477294921875,
      "mean_similarity": 0.2502909302711487
    },
    {
      "video_id": "215",
      "question_id": "215-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What is the ending pose of the team at the end of this video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 1.7399747371673584,
      "inference_time": 4.1512415409088135,
      "total_chunks": 10,
      "total_frames_used": 70,
      "max_chunk_length": 10,
      "avg_chunk_length": 7.0,
      "top_similarity": 0.3347756862640381,
      "mean_similarity": 0.31888216733932495
    },
    {
      "video_id": "215",
      "question_id": "215-3",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many individuals are in the team, with each person dressed in yellow?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 1.7566406726837158,
      "inference_time": 4.468780994415283,
      "total_chunks": 10,
      "total_frames_used": 65,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.5,
      "top_similarity": 0.25207602977752686,
      "mean_similarity": 0.2347499132156372
    },
    {
      "video_id": "413",
      "question_id": "413-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What is \"Avenues for Justice\" mentioned in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 8.597474813461304,
      "inference_time": 4.244915723800659,
      "total_chunks": 10,
      "total_frames_used": 110,
      "max_chunk_length": 30,
      "avg_chunk_length": 11.0,
      "top_similarity": 0.3036395013332367,
      "mean_similarity": 0.28589552640914917
    },
    {
      "video_id": "413",
      "question_id": "413-2",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is this video about?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 8.480466842651367,
      "inference_time": 4.206937074661255,
      "total_chunks": 10,
      "total_frames_used": 120,
      "max_chunk_length": 30,
      "avg_chunk_length": 12.0,
      "top_similarity": 0.2998065948486328,
      "mean_similarity": 0.2932683825492859
    },
    {
      "video_id": "413",
      "question_id": "413-3",
      "task_type": "Temporal Perception",
      "duration": "medium",
      "question": "In which part of the video is the woman in the blue top interviewed?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 8.527323246002197,
      "inference_time": 5.8110716342926025,
      "total_chunks": 10,
      "total_frames_used": 240,
      "max_chunk_length": 30,
      "avg_chunk_length": 24.0,
      "top_similarity": 0.29059895873069763,
      "mean_similarity": 0.278702974319458
    },
    {
      "video_id": "530",
      "question_id": "530-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which of the following materials is not used in this video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 5.01164698600769,
      "inference_time": 4.852890968322754,
      "total_chunks": 10,
      "total_frames_used": 235,
      "max_chunk_length": 30,
      "avg_chunk_length": 23.5,
      "top_similarity": 0.29335102438926697,
      "mean_similarity": 0.28650325536727905
    },
    {
      "video_id": "530",
      "question_id": "530-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many stars can be extracted from one CD?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 4.842321157455444,
      "inference_time": 5.1537933349609375,
      "total_chunks": 10,
      "total_frames_used": 235,
      "max_chunk_length": 30,
      "avg_chunk_length": 23.5,
      "top_similarity": 0.3115336298942566,
      "mean_similarity": 0.2878367304801941
    },
    {
      "video_id": "530",
      "question_id": "530-3",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is this video mainly about?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 4.878814458847046,
      "inference_time": 5.913762807846069,
      "total_chunks": 10,
      "total_frames_used": 235,
      "max_chunk_length": 30,
      "avg_chunk_length": 23.5,
      "top_similarity": 0.28201615810394287,
      "mean_similarity": 0.2738505005836487
    },
    {
      "video_id": "443",
      "question_id": "443-1",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many attempts do it take for Phonzy to hit a 3-pointer in the first game?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 6.587676048278809,
      "inference_time": 5.04013466835022,
      "total_chunks": 10,
      "total_frames_used": 120,
      "max_chunk_length": 25,
      "avg_chunk_length": 12.0,
      "top_similarity": 0.3174915909767151,
      "mean_similarity": 0.30290478467941284
    },
    {
      "video_id": "443",
      "question_id": "443-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many free throws does Jamal hit in game 2?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 6.6125664710998535,
      "inference_time": 4.810876131057739,
      "total_chunks": 10,
      "total_frames_used": 125,
      "max_chunk_length": 25,
      "avg_chunk_length": 12.5,
      "top_similarity": 0.29391905665397644,
      "mean_similarity": 0.2783448100090027
    },
    {
      "video_id": "443",
      "question_id": "443-3",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "Why is there no winner in Game 3?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 6.605451345443726,
      "inference_time": 4.425454616546631,
      "total_chunks": 10,
      "total_frames_used": 125,
      "max_chunk_length": 25,
      "avg_chunk_length": 12.5,
      "top_similarity": 0.288335382938385,
      "mean_similarity": 0.2757313847541809
    },
    {
      "video_id": "304",
      "question_id": "304-1",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "Why did the main character in the video put his hand in the coat?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 8.425208806991577,
      "inference_time": 3.3274762630462646,
      "total_chunks": 10,
      "total_frames_used": 55,
      "max_chunk_length": 10,
      "avg_chunk_length": 5.5,
      "top_similarity": 0.31416618824005127,
      "mean_similarity": 0.30476588010787964
    },
    {
      "video_id": "304",
      "question_id": "304-2",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "Who started the custom of restraining hand activities in pulic?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 8.262223958969116,
      "inference_time": 3.3432791233062744,
      "total_chunks": 10,
      "total_frames_used": 75,
      "max_chunk_length": 25,
      "avg_chunk_length": 7.5,
      "top_similarity": 0.264636754989624,
      "mean_similarity": 0.2557379901409149
    },
    {
      "video_id": "304",
      "question_id": "304-3",
      "task_type": "Spatial Perception",
      "duration": "medium",
      "question": "Which of the following elements isn't mentioned in the painting \"The Emperor Napoleon in His Study at the Tuileries\"?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 8.357736349105835,
      "inference_time": 3.357977867126465,
      "total_chunks": 10,
      "total_frames_used": 50,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.34749627113342285,
      "mean_similarity": 0.3256680965423584
    },
    {
      "video_id": "382",
      "question_id": "382-1",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "According to the video, why is IMAX not suitable for filming movies?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 6.061591386795044,
      "inference_time": 4.0464887619018555,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 15,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.36344054341316223,
      "mean_similarity": 0.33705464005470276
    },
    {
      "video_id": "382",
      "question_id": "382-2",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "According to the video, which statement is correct?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 5.979879379272461,
      "inference_time": 4.059594631195068,
      "total_chunks": 10,
      "total_frames_used": 80,
      "max_chunk_length": 25,
      "avg_chunk_length": 8.0,
      "top_similarity": 0.2892252802848816,
      "mean_similarity": 0.28197044134140015
    },
    {
      "video_id": "382",
      "question_id": "382-3",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which IMAX movie isn't in the video?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 5.9963977336883545,
      "inference_time": 3.9975943565368652,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 15,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3534550666809082,
      "mean_similarity": 0.3298497200012207
    },
    {
      "video_id": "517",
      "question_id": "517-1",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many female performers are doing this show?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 7.488074064254761,
      "inference_time": 5.043170928955078,
      "total_chunks": 10,
      "total_frames_used": 265,
      "max_chunk_length": 30,
      "avg_chunk_length": 26.5,
      "top_similarity": 0.2782105505466461,
      "mean_similarity": 0.265221506357193
    },
    {
      "video_id": "517",
      "question_id": "517-2",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "What happened after the two actresses climbed onto the high board?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 7.15991735458374,
      "inference_time": 5.183476686477661,
      "total_chunks": 10,
      "total_frames_used": 275,
      "max_chunk_length": 30,
      "avg_chunk_length": 27.5,
      "top_similarity": 0.2853379249572754,
      "mean_similarity": 0.2787643074989319
    },
    {
      "video_id": "517",
      "question_id": "517-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What is the function of the actor hanging on the left swing?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 7.26028299331665,
      "inference_time": 5.168006420135498,
      "total_chunks": 10,
      "total_frames_used": 275,
      "max_chunk_length": 30,
      "avg_chunk_length": 27.5,
      "top_similarity": 0.3044995963573456,
      "mean_similarity": 0.2967873215675354
    },
    {
      "video_id": "475",
      "question_id": "475-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What sport are the two athletes playing?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 4.17614221572876,
      "inference_time": 4.941948890686035,
      "total_chunks": 10,
      "total_frames_used": 100,
      "max_chunk_length": 25,
      "avg_chunk_length": 10.0,
      "top_similarity": 0.28337037563323975,
      "mean_similarity": 0.27580463886260986
    },
    {
      "video_id": "475",
      "question_id": "475-2",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "How many points does the winner get enough to win the match?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 4.14782452583313,
      "inference_time": 5.7725512981414795,
      "total_chunks": 10,
      "total_frames_used": 95,
      "max_chunk_length": 25,
      "avg_chunk_length": 9.5,
      "top_similarity": 0.3005290925502777,
      "mean_similarity": 0.29403021931648254
    },
    {
      "video_id": "475",
      "question_id": "475-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What do the two players have in common?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 4.1016576290130615,
      "inference_time": 4.961309909820557,
      "total_chunks": 10,
      "total_frames_used": 100,
      "max_chunk_length": 25,
      "avg_chunk_length": 10.0,
      "top_similarity": 0.25888147950172424,
      "mean_similarity": 0.25177788734436035
    },
    {
      "video_id": "380",
      "question_id": "380-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "As can be seen in the video, which food is not used to practice holding chopsticks?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 4.632862329483032,
      "inference_time": 4.898355722427368,
      "total_chunks": 10,
      "total_frames_used": 160,
      "max_chunk_length": 30,
      "avg_chunk_length": 16.0,
      "top_similarity": 0.37379318475723267,
      "mean_similarity": 0.3412338197231293
    },
    {
      "video_id": "380",
      "question_id": "380-2",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "As depicted in the video, which finger touches the chopsticks?",
      "predicted": "A",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 4.586488485336304,
      "inference_time": 4.064174175262451,
      "total_chunks": 10,
      "total_frames_used": 155,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.5,
      "top_similarity": 0.3870830833911896,
      "mean_similarity": 0.36077865958213806
    },
    {
      "video_id": "380",
      "question_id": "380-3",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is this video mainly about?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 4.630587577819824,
      "inference_time": 4.762507915496826,
      "total_chunks": 10,
      "total_frames_used": 150,
      "max_chunk_length": 30,
      "avg_chunk_length": 15.0,
      "top_similarity": 0.28995034098625183,
      "mean_similarity": 0.27027562260627747
    },
    {
      "video_id": "411",
      "question_id": "411-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What caused the dilapidated scene at the beginning of the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 9.593401670455933,
      "inference_time": 4.899442195892334,
      "total_chunks": 10,
      "total_frames_used": 145,
      "max_chunk_length": 30,
      "avg_chunk_length": 14.5,
      "top_similarity": 0.3105747103691101,
      "mean_similarity": 0.29596155881881714
    },
    {
      "video_id": "411",
      "question_id": "411-2",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "What are the children wearing pink outfits doing at the beginning of the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 9.359055519104004,
      "inference_time": 4.721251726150513,
      "total_chunks": 10,
      "total_frames_used": 180,
      "max_chunk_length": 30,
      "avg_chunk_length": 18.0,
      "top_similarity": 0.3399085998535156,
      "mean_similarity": 0.31739991903305054
    },
    {
      "video_id": "411",
      "question_id": "411-3",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "In the middle of the video, what are the difficulties of rebuilding after the earthquake?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 9.459803104400635,
      "inference_time": 4.199842929840088,
      "total_chunks": 10,
      "total_frames_used": 115,
      "max_chunk_length": 30,
      "avg_chunk_length": 11.5,
      "top_similarity": 0.35333162546157837,
      "mean_similarity": 0.3374204933643341
    },
    {
      "video_id": "473",
      "question_id": "473-1",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many men's single matches are included in this video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 4.750164031982422,
      "inference_time": 4.956600904464722,
      "total_chunks": 10,
      "total_frames_used": 110,
      "max_chunk_length": 30,
      "avg_chunk_length": 11.0,
      "top_similarity": 0.3106285333633423,
      "mean_similarity": 0.3033389151096344
    },
    {
      "video_id": "473",
      "question_id": "473-2",
      "task_type": "Spatial Perception",
      "duration": "medium",
      "question": "Where is the first match held?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 4.6084840297698975,
      "inference_time": 4.963076829910278,
      "total_chunks": 10,
      "total_frames_used": 135,
      "max_chunk_length": 30,
      "avg_chunk_length": 13.5,
      "top_similarity": 0.29014191031455994,
      "mean_similarity": 0.2833574116230011
    },
    {
      "video_id": "473",
      "question_id": "473-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "Which team won the first men's doubles?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 4.614955425262451,
      "inference_time": 4.97612738609314,
      "total_chunks": 10,
      "total_frames_used": 165,
      "max_chunk_length": 30,
      "avg_chunk_length": 16.5,
      "top_similarity": 0.28975722193717957,
      "mean_similarity": 0.28392294049263
    },
    {
      "video_id": "353",
      "question_id": "353-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What's the weather like in the city at the beginning of the video?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 11.36358380317688,
      "inference_time": 4.033956289291382,
      "total_chunks": 10,
      "total_frames_used": 75,
      "max_chunk_length": 25,
      "avg_chunk_length": 7.5,
      "top_similarity": 0.31438717246055603,
      "mean_similarity": 0.2951412796974182
    },
    {
      "video_id": "353",
      "question_id": "353-2",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What is the color of the clothes that the speaker is wearing in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 11.026660919189453,
      "inference_time": 4.785632610321045,
      "total_chunks": 10,
      "total_frames_used": 145,
      "max_chunk_length": 30,
      "avg_chunk_length": 14.5,
      "top_similarity": 0.2821097671985626,
      "mean_similarity": 0.2742813527584076
    },
    {
      "video_id": "353",
      "question_id": "353-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What does the speaker explain before introducing MS4?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 11.097182512283325,
      "inference_time": 5.944501638412476,
      "total_chunks": 10,
      "total_frames_used": 170,
      "max_chunk_length": 30,
      "avg_chunk_length": 17.0,
      "top_similarity": 0.2744141221046448,
      "mean_similarity": 0.2664799094200134
    },
    {
      "video_id": "348",
      "question_id": "348-1",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "Which specific phenomenon during the video demonstrated the correctness of general relativity?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 6.244165658950806,
      "inference_time": 4.232252597808838,
      "total_chunks": 10,
      "total_frames_used": 55,
      "max_chunk_length": 10,
      "avg_chunk_length": 5.5,
      "top_similarity": 0.3383390009403229,
      "mean_similarity": 0.3023776412010193
    },
    {
      "video_id": "348",
      "question_id": "348-2",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is the video about?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 6.154452085494995,
      "inference_time": 4.064769506454468,
      "total_chunks": 10,
      "total_frames_used": 75,
      "max_chunk_length": 25,
      "avg_chunk_length": 7.5,
      "top_similarity": 0.279274046421051,
      "mean_similarity": 0.2739657461643219
    },
    {
      "video_id": "348",
      "question_id": "348-3",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "At least how many total solar eclipses occurred between 1921 and 1970 according to the video?",
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 6.091039419174194,
      "inference_time": 3.313599109649658,
      "total_chunks": 10,
      "total_frames_used": 75,
      "max_chunk_length": 15,
      "avg_chunk_length": 7.5,
      "top_similarity": 0.36654719710350037,
      "mean_similarity": 0.3469122052192688
    },
    {
      "video_id": "495",
      "question_id": "495-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What type of cards does the magician ask the female judge to pick one from?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 10.249813318252563,
      "inference_time": 4.676542043685913,
      "total_chunks": 10,
      "total_frames_used": 120,
      "max_chunk_length": 30,
      "avg_chunk_length": 12.0,
      "top_similarity": 0.32626476883888245,
      "mean_similarity": 0.3019329905509949
    },
    {
      "video_id": "495",
      "question_id": "495-2",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "What magic does the magician first perform on the stage?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 10.20361590385437,
      "inference_time": 5.086759328842163,
      "total_chunks": 10,
      "total_frames_used": 115,
      "max_chunk_length": 30,
      "avg_chunk_length": 11.5,
      "top_similarity": 0.31988731026649475,
      "mean_similarity": 0.30423492193222046
    },
    {
      "video_id": "495",
      "question_id": "495-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What do the two performances have in common?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 10.388255596160889,
      "inference_time": 4.7446160316467285,
      "total_chunks": 10,
      "total_frames_used": 110,
      "max_chunk_length": 30,
      "avg_chunk_length": 11.0,
      "top_similarity": 0.2770423889160156,
      "mean_similarity": 0.27013587951660156
    },
    {
      "video_id": "350",
      "question_id": "350-1",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is the primary topic of the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 6.056538105010986,
      "inference_time": 3.3419063091278076,
      "total_chunks": 10,
      "total_frames_used": 50,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.2954414188861847,
      "mean_similarity": 0.2673412561416626
    },
    {
      "video_id": "350",
      "question_id": "350-2",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "How does the diameter of the black hole S5 0014+81 compare to other distances in space?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 6.033020496368408,
      "inference_time": 3.3395750522613525,
      "total_chunks": 10,
      "total_frames_used": 55,
      "max_chunk_length": 10,
      "avg_chunk_length": 5.5,
      "top_similarity": 0.3188626170158386,
      "mean_similarity": 0.30027496814727783
    },
    {
      "video_id": "350",
      "question_id": "350-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What indicates if we travel inside a black hole for quite a while?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 6.036137819290161,
      "inference_time": 3.3499622344970703,
      "total_chunks": 10,
      "total_frames_used": 50,
      "max_chunk_length": 5,
      "avg_chunk_length": 5.0,
      "top_similarity": 0.3330119848251343,
      "mean_similarity": 0.31009727716445923
    },
    {
      "video_id": "484",
      "question_id": "484-1",
      "task_type": "Spatial Reasoning",
      "duration": "medium",
      "question": "What is the theme of the performance in the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 4.0550477504730225,
      "inference_time": 5.107727289199829,
      "total_chunks": 10,
      "total_frames_used": 225,
      "max_chunk_length": 30,
      "avg_chunk_length": 22.5,
      "top_similarity": 0.2874728739261627,
      "mean_similarity": 0.2783631384372711
    },
    {
      "video_id": "484",
      "question_id": "484-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many seats are on stage in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 3.9532179832458496,
      "inference_time": 6.0587780475616455,
      "total_chunks": 10,
      "total_frames_used": 230,
      "max_chunk_length": 30,
      "avg_chunk_length": 23.0,
      "top_similarity": 0.3005223572254181,
      "mean_similarity": 0.29258039593696594
    },
    {
      "video_id": "484",
      "question_id": "484-3",
      "task_type": "Temporal Perception",
      "duration": "medium",
      "question": "What can be inferred about the story at the beginning of the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 3.956007957458496,
      "inference_time": 5.113822937011719,
      "total_chunks": 10,
      "total_frames_used": 145,
      "max_chunk_length": 30,
      "avg_chunk_length": 14.5,
      "top_similarity": 0.28182661533355713,
      "mean_similarity": 0.26502740383148193
    },
    {
      "video_id": "477",
      "question_id": "477-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which team participates in the first five matches in this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 10.1269052028656,
      "inference_time": 4.121400833129883,
      "total_chunks": 10,
      "total_frames_used": 95,
      "max_chunk_length": 25,
      "avg_chunk_length": 9.5,
      "top_similarity": 0.3209982216358185,
      "mean_similarity": 0.31682339310646057
    },
    {
      "video_id": "477",
      "question_id": "477-2",
      "task_type": "OCR Problems",
      "duration": "medium",
      "question": "What is the number written on the back of Nishida?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 10.035468101501465,
      "inference_time": 3.3535287380218506,
      "total_chunks": 10,
      "total_frames_used": 95,
      "max_chunk_length": 25,
      "avg_chunk_length": 9.5,
      "top_similarity": 0.305387020111084,
      "mean_similarity": 0.28020215034484863
    },
    {
      "video_id": "477",
      "question_id": "477-3",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "How does JPN team usually win a point in this video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 9.99525260925293,
      "inference_time": 4.056667804718018,
      "total_chunks": 10,
      "total_frames_used": 80,
      "max_chunk_length": 20,
      "avg_chunk_length": 8.0,
      "top_similarity": 0.3430827558040619,
      "mean_similarity": 0.3366318643093109
    },
    {
      "video_id": "436",
      "question_id": "436-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What can be seen in the sky during the race?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 9.25933027267456,
      "inference_time": 4.4049365520477295,
      "total_chunks": 10,
      "total_frames_used": 160,
      "max_chunk_length": 30,
      "avg_chunk_length": 16.0,
      "top_similarity": 0.3021819293498993,
      "mean_similarity": 0.2925935387611389
    },
    {
      "video_id": "436",
      "question_id": "436-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many cars does player use?",
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 9.010791063308716,
      "inference_time": 4.159024238586426,
      "total_chunks": 10,
      "total_frames_used": 135,
      "max_chunk_length": 30,
      "avg_chunk_length": 13.5,
      "top_similarity": 0.29558175802230835,
      "mean_similarity": 0.2870253920555115
    },
    {
      "video_id": "436",
      "question_id": "436-3",
      "task_type": "Spatial Perception",
      "duration": "medium",
      "question": "What is the weather like?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 9.035014629364014,
      "inference_time": 3.336810827255249,
      "total_chunks": 10,
      "total_frames_used": 65,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.5,
      "top_similarity": 0.250995397567749,
      "mean_similarity": 0.23571646213531494
    },
    {
      "video_id": "323",
      "question_id": "323-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which cellular structure is responsible for receiving proteins according to the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 6.756707429885864,
      "inference_time": 4.90752387046814,
      "total_chunks": 10,
      "total_frames_used": 110,
      "max_chunk_length": 20,
      "avg_chunk_length": 11.0,
      "top_similarity": 0.3413469195365906,
      "mean_similarity": 0.325344979763031
    },
    {
      "video_id": "323",
      "question_id": "323-2",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "Which components are part of the object described in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 6.67385721206665,
      "inference_time": 5.772064924240112,
      "total_chunks": 10,
      "total_frames_used": 80,
      "max_chunk_length": 10,
      "avg_chunk_length": 8.0,
      "top_similarity": 0.2919503450393677,
      "mean_similarity": 0.2823140025138855
    },
    {
      "video_id": "323",
      "question_id": "323-3",
      "task_type": "Temporal Reasoning",
      "duration": "medium",
      "question": "What is the correct chronological order in which the following parts of the video appear?\n(a) Human lungs.\n(b) Protein folding distortion.\n(c) Mice, plants and cells.",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 6.70506477355957,
      "inference_time": 5.67322301864624,
      "total_chunks": 10,
      "total_frames_used": 120,
      "max_chunk_length": 20,
      "avg_chunk_length": 12.0,
      "top_similarity": 0.3052971363067627,
      "mean_similarity": 0.2968943119049072
    },
    {
      "video_id": "536",
      "question_id": "536-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What is the first food the main character in the video tried?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 7.722122430801392,
      "inference_time": 5.039807319641113,
      "total_chunks": 10,
      "total_frames_used": 205,
      "max_chunk_length": 30,
      "avg_chunk_length": 20.5,
      "top_similarity": 0.32317566871643066,
      "mean_similarity": 0.31362390518188477
    },
    {
      "video_id": "536",
      "question_id": "536-2",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "Of the following foods, what does the protagonist in the video prefer?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 7.473175525665283,
      "inference_time": 4.272366523742676,
      "total_chunks": 10,
      "total_frames_used": 165,
      "max_chunk_length": 30,
      "avg_chunk_length": 16.5,
      "top_similarity": 0.3122789263725281,
      "mean_similarity": 0.29582521319389343
    },
    {
      "video_id": "536",
      "question_id": "536-3",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "The ad in the video is inserted while the main character is eating what?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 7.491991996765137,
      "inference_time": 4.815073490142822,
      "total_chunks": 10,
      "total_frames_used": 170,
      "max_chunk_length": 30,
      "avg_chunk_length": 17.0,
      "top_similarity": 0.3208911716938019,
      "mean_similarity": 0.3087349832057953
    },
    {
      "video_id": "575",
      "question_id": "575-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What are the animals featured in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 3.2370336055755615,
      "inference_time": 4.975586652755737,
      "total_chunks": 10,
      "total_frames_used": 200,
      "max_chunk_length": 30,
      "avg_chunk_length": 20.0,
      "top_similarity": 0.3014165461063385,
      "mean_similarity": 0.29160821437835693
    },
    {
      "video_id": "575",
      "question_id": "575-2",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "What color is the house?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 3.2229413986206055,
      "inference_time": 5.911889553070068,
      "total_chunks": 10,
      "total_frames_used": 200,
      "max_chunk_length": 30,
      "avg_chunk_length": 20.0,
      "top_similarity": 0.23687633872032166,
      "mean_similarity": 0.2192404568195343
    },
    {
      "video_id": "575",
      "question_id": "575-3",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What is between the yellow fox and the white fox?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 3.213513135910034,
      "inference_time": 4.984951496124268,
      "total_chunks": 10,
      "total_frames_used": 200,
      "max_chunk_length": 30,
      "avg_chunk_length": 20.0,
      "top_similarity": 0.2811588644981384,
      "mean_similarity": 0.2655400335788727
    },
    {
      "video_id": "664",
      "question_id": "664-1",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is the video mainly about?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 26.11654806137085,
      "inference_time": 3.9195897579193115,
      "total_chunks": 10,
      "total_frames_used": 160,
      "max_chunk_length": 60,
      "avg_chunk_length": 16.0,
      "top_similarity": 0.2877429127693176,
      "mean_similarity": 0.2658824324607849
    },
    {
      "video_id": "664",
      "question_id": "664-2",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "Who is the biological father of the girl in the second case in the video?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 25.95064878463745,
      "inference_time": 4.138067245483398,
      "total_chunks": 10,
      "total_frames_used": 120,
      "max_chunk_length": 25,
      "avg_chunk_length": 12.0,
      "top_similarity": 0.3103299140930176,
      "mean_similarity": 0.2987430691719055
    },
    {
      "video_id": "664",
      "question_id": "664-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the view held by the female witness present in the first case in the video?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 26.01389789581299,
      "inference_time": 4.151561498641968,
      "total_chunks": 10,
      "total_frames_used": 175,
      "max_chunk_length": 40,
      "avg_chunk_length": 17.5,
      "top_similarity": 0.32764261960983276,
      "mean_similarity": 0.31196945905685425
    },
    {
      "video_id": "794",
      "question_id": "794-1",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is the primary focus of this video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 33.4268639087677,
      "inference_time": 4.811914682388306,
      "total_chunks": 10,
      "total_frames_used": 95,
      "max_chunk_length": 20,
      "avg_chunk_length": 9.5,
      "top_similarity": 0.2980484068393707,
      "mean_similarity": 0.2880910336971283
    },
    {
      "video_id": "794",
      "question_id": "794-2",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "What is the key to the illusion of bending a teaspoon with his mind?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 33.310707330703735,
      "inference_time": 4.740294456481934,
      "total_chunks": 10,
      "total_frames_used": 235,
      "max_chunk_length": 60,
      "avg_chunk_length": 23.5,
      "top_similarity": 0.26707449555397034,
      "mean_similarity": 0.2587505578994751
    },
    {
      "video_id": "794",
      "question_id": "794-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the purpose of the mirrors in the last magic?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 32.81999111175537,
      "inference_time": 3.9190444946289062,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3041159212589264,
      "mean_similarity": 0.29331380128860474
    },
    {
      "video_id": "641",
      "question_id": "641-1",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What future human activity does the video discuss?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 45.15802335739136,
      "inference_time": 4.813357830047607,
      "total_chunks": 10,
      "total_frames_used": 120,
      "max_chunk_length": 25,
      "avg_chunk_length": 12.0,
      "top_similarity": 0.28561896085739136,
      "mean_similarity": 0.2757413387298584
    },
    {
      "video_id": "641",
      "question_id": "641-2",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "In what order are the following planets introduced in the video?",
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 45.09566259384155,
      "inference_time": 3.230799436569214,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 15,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.3271656036376953,
      "mean_similarity": 0.3206418454647064
    },
    {
      "video_id": "641",
      "question_id": "641-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What are the planets that Dr.David Grinspoon and Dr. Heidi B. Hammel research?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 45.00009202957153,
      "inference_time": 3.9115352630615234,
      "total_chunks": 10,
      "total_frames_used": 65,
      "max_chunk_length": 20,
      "avg_chunk_length": 6.5,
      "top_similarity": 0.3033055365085602,
      "mean_similarity": 0.29580992460250854
    },
    {
      "video_id": "883",
      "question_id": "883-1",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "Which move in the video was done in both the warm-up phase and the official workout phase of Monday's workout?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 27.916008949279785,
      "inference_time": 5.973596572875977,
      "total_chunks": 10,
      "total_frames_used": 485,
      "max_chunk_length": 60,
      "avg_chunk_length": 48.5,
      "top_similarity": 0.27311933040618896,
      "mean_similarity": 0.269959032535553
    },
    {
      "video_id": "883",
      "question_id": "883-2",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the difference in appearance between the female lead in the video, Friday and Wednesday?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 28.08217978477478,
      "inference_time": 3.9068727493286133,
      "total_chunks": 10,
      "total_frames_used": 235,
      "max_chunk_length": 60,
      "avg_chunk_length": 23.5,
      "top_similarity": 0.2883261442184448,
      "mean_similarity": 0.2655998468399048
    },
    {
      "video_id": "883",
      "question_id": "883-3",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "Which of the following describes the heroine's weekly fitness programme correctly?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 28.17731285095215,
      "inference_time": 4.845510005950928,
      "total_chunks": 10,
      "total_frames_used": 545,
      "max_chunk_length": 60,
      "avg_chunk_length": 54.5,
      "top_similarity": 0.2780539393424988,
      "mean_similarity": 0.2651628255844116
    },
    {
      "video_id": "751",
      "question_id": "751-1",
      "task_type": "Counting Problem",
      "duration": "long",
      "question": "How many goals did the number 9 of the blue team score in the match?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 28.76051163673401,
      "inference_time": 3.9409592151641846,
      "total_chunks": 10,
      "total_frames_used": 80,
      "max_chunk_length": 20,
      "avg_chunk_length": 8.0,
      "top_similarity": 0.3097178339958191,
      "mean_similarity": 0.29415374994277954
    },
    {
      "video_id": "751",
      "question_id": "751-2",
      "task_type": "Counting Problem",
      "duration": "long",
      "question": "How many timeout substitutions were there during the game in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 28.51408076286316,
      "inference_time": 3.2372076511383057,
      "total_chunks": 10,
      "total_frames_used": 90,
      "max_chunk_length": 25,
      "avg_chunk_length": 9.0,
      "top_similarity": 0.3063490390777588,
      "mean_similarity": 0.28472453355789185
    },
    {
      "video_id": "751",
      "question_id": "751-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "How did the match in the video progress in terms of scoring, leading up to its conclusion?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 28.439823150634766,
      "inference_time": 4.208193302154541,
      "total_chunks": 10,
      "total_frames_used": 95,
      "max_chunk_length": 20,
      "avg_chunk_length": 9.5,
      "top_similarity": 0.31511932611465454,
      "mean_similarity": 0.30694839358329773
    },
    {
      "video_id": "786",
      "question_id": "786-1",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "How does Snow White come to meet the seven miners?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 23.42195200920105,
      "inference_time": 4.966756582260132,
      "total_chunks": 10,
      "total_frames_used": 420,
      "max_chunk_length": 60,
      "avg_chunk_length": 42.0,
      "top_similarity": 0.31250765919685364,
      "mean_similarity": 0.30407118797302246
    },
    {
      "video_id": "786",
      "question_id": "786-2",
      "task_type": "Action Recognition",
      "duration": "long",
      "question": "What is the Queen's fate in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 22.35141611099243,
      "inference_time": 4.6651527881622314,
      "total_chunks": 10,
      "total_frames_used": 230,
      "max_chunk_length": 60,
      "avg_chunk_length": 23.0,
      "top_similarity": 0.2858561873435974,
      "mean_similarity": 0.28183847665786743
    },
    {
      "video_id": "786",
      "question_id": "786-3",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "What inference can be made about the actress on her interactions in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 22.701916456222534,
      "inference_time": 4.666667699813843,
      "total_chunks": 10,
      "total_frames_used": 330,
      "max_chunk_length": 60,
      "avg_chunk_length": 33.0,
      "top_similarity": 0.2716357707977295,
      "mean_similarity": 0.26837167143821716
    },
    {
      "video_id": "896",
      "question_id": "896-1",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What sets apart the third set of stickers from the other two?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 25.192179679870605,
      "inference_time": 5.987450361251831,
      "total_chunks": 10,
      "total_frames_used": 440,
      "max_chunk_length": 60,
      "avg_chunk_length": 44.0,
      "top_similarity": 0.2693515121936798,
      "mean_similarity": 0.2621018588542938
    },
    {
      "video_id": "896",
      "question_id": "896-2",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "What is the proper sequence for the following items used in this video?\n(a) Stickers.\n(b) Watercolor pencils.\n(c) Gems.\n(d) Glue paper.",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 24.37416172027588,
      "inference_time": 5.831282377243042,
      "total_chunks": 10,
      "total_frames_used": 505,
      "max_chunk_length": 60,
      "avg_chunk_length": 50.5,
      "top_similarity": 0.32394132018089294,
      "mean_similarity": 0.30745765566825867
    },
    {
      "video_id": "896",
      "question_id": "896-3",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is captured in this video?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 23.970693588256836,
      "inference_time": 5.752534866333008,
      "total_chunks": 10,
      "total_frames_used": 325,
      "max_chunk_length": 60,
      "avg_chunk_length": 32.5,
      "top_similarity": 0.2886068522930145,
      "mean_similarity": 0.28323793411254883
    },
    {
      "video_id": "699",
      "question_id": "699-1",
      "task_type": "Action Recognition",
      "duration": "long",
      "question": "In line with the video evidence, what does the team do in the first story?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 33.728726387023926,
      "inference_time": 3.2251386642456055,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 15,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.27724283933639526,
      "mean_similarity": 0.2686872184276581
    },
    {
      "video_id": "699",
      "question_id": "699-2",
      "task_type": "Object Recognition",
      "duration": "long",
      "question": "In accordance with the video footage, who protects the mermaid?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 33.581594944000244,
      "inference_time": 3.88554048538208,
      "total_chunks": 10,
      "total_frames_used": 70,
      "max_chunk_length": 15,
      "avg_chunk_length": 7.0,
      "top_similarity": 0.30543696880340576,
      "mean_similarity": 0.29840803146362305
    },
    {
      "video_id": "699",
      "question_id": "699-3",
      "task_type": "Action Recognition",
      "duration": "long",
      "question": "What happens when the figures in the video are electrocuted?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 33.73568892478943,
      "inference_time": 3.2324318885803223,
      "total_chunks": 10,
      "total_frames_used": 55,
      "max_chunk_length": 10,
      "avg_chunk_length": 5.5,
      "top_similarity": 0.28439897298812866,
      "mean_similarity": 0.2760617434978485
    },
    {
      "video_id": "636",
      "question_id": "636-1",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "Which of the following best summarizes the main concern of those who advocate for a reduction in the balance sheet in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 35.9105224609375,
      "inference_time": 4.86271333694458,
      "total_chunks": 10,
      "total_frames_used": 85,
      "max_chunk_length": 25,
      "avg_chunk_length": 8.5,
      "top_similarity": 0.31168609857559204,
      "mean_similarity": 0.3042747378349304
    },
    {
      "video_id": "636",
      "question_id": "636-2",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "According to the video, what is one reason why the relationship between inflation and wage inflation has weakened in recent decades?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 35.94505167007446,
      "inference_time": 3.944821834564209,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.36697903275489807,
      "mean_similarity": 0.3516841232776642
    },
    {
      "video_id": "636",
      "question_id": "636-3",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is the main focus of this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 36.281124114990234,
      "inference_time": 4.4689977169036865,
      "total_chunks": 10,
      "total_frames_used": 155,
      "max_chunk_length": 40,
      "avg_chunk_length": 15.5,
      "top_similarity": 0.30573567748069763,
      "mean_similarity": 0.29743653535842896
    },
    {
      "video_id": "624",
      "question_id": "624-1",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the speaker's opinion on reversing type 2 diabetes?",
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 21.93523120880127,
      "inference_time": 4.816108465194702,
      "total_chunks": 10,
      "total_frames_used": 360,
      "max_chunk_length": 60,
      "avg_chunk_length": 36.0,
      "top_similarity": 0.3430033326148987,
      "mean_similarity": 0.32356181740760803
    },
    {
      "video_id": "624",
      "question_id": "624-2",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "What is the order in which the following is introduced in the video?\n\u2460 Insulin Resistance Explained.\n\u2461 Case studies.\n\u2462 How to Reverse Type 2 Diabetes.",
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 21.719967365264893,
      "inference_time": 4.802783727645874,
      "total_chunks": 10,
      "total_frames_used": 405,
      "max_chunk_length": 60,
      "avg_chunk_length": 40.5,
      "top_similarity": 0.3577224016189575,
      "mean_similarity": 0.3312914967536926
    },
    {
      "video_id": "624",
      "question_id": "624-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What treatment is universally implemented in all the case studies featured in the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 21.787009954452515,
      "inference_time": 4.039400339126587,
      "total_chunks": 10,
      "total_frames_used": 320,
      "max_chunk_length": 60,
      "avg_chunk_length": 32.0,
      "top_similarity": 0.3280688524246216,
      "mean_similarity": 0.3048727810382843
    },
    {
      "video_id": "001",
      "question_id": "001-1",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?",
      "predicted": "A",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 1.5507445335388184,
      "inference_time": 4.138596296310425,
      "total_chunks": 10,
      "total_frames_used": 60,
      "max_chunk_length": 10,
      "avg_chunk_length": 6.0,
      "top_similarity": 0.31728023290634155,
      "mean_similarity": 0.2942323386669159
    },
    {
      "video_id": "001",
      "question_id": "001-2",
      "task_type": "Information Synopsis",
      "duration": "short",
      "question": "What is the genre of this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 1.4451196193695068,
      "inference_time": 3.2478909492492676,
      "total_chunks": 10,
      "total_frames_used": 70,
      "max_chunk_length": 15,
      "avg_chunk_length": 7.0,
      "top_similarity": 0.2610328793525696,
      "mean_similarity": 0.23433859646320343
    },
    {
      "video_id": "001",
      "question_id": "001-3",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many red socks are above the fireplace at the end of this video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 1.4437737464904785,
      "inference_time": 1.0285537242889404,
      "total_chunks": 10,
      "total_frames_used": 70,
      "max_chunk_length": 15,
      "avg_chunk_length": 7.0,
      "top_similarity": 0.33883851766586304,
      "mean_similarity": 0.26197466254234314
    }
  ]
}