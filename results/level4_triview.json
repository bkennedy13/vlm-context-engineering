{
  "level": "level4_triview",
  "config": {
    "k": 10,
    "frame_budget": 30,
    "frame_pooling": "mean",
    "rrf_k": 60,
    "retrieval": "triview_rrf",
    "answering_model": "Qwen2.5-VL-7B-Instruct"
  },
  "partial": false,
  "total_questions": 150,
  "correct": 89,
  "accuracy": 0.5933333333333334,
  "avg_retrieval_time": 0.025393751462300617,
  "avg_inference_time": 12.3313733959198,
  "total_time": 1864.05202126503,
  "task_type_stats": {
    "Spatial Reasoning": {
      "total": 4,
      "correct": 4,
      "accuracy": 1.0
    },
    "Spatial Perception": {
      "total": 5,
      "correct": 4,
      "accuracy": 0.8
    },
    "Object Recognition": {
      "total": 23,
      "correct": 18,
      "accuracy": 0.782608695652174
    },
    "Temporal Reasoning": {
      "total": 7,
      "correct": 3,
      "accuracy": 0.42857142857142855
    },
    "Action Reasoning": {
      "total": 14,
      "correct": 8,
      "accuracy": 0.5714285714285714
    },
    "Information Synopsis": {
      "total": 15,
      "correct": 12,
      "accuracy": 0.8
    },
    "Action Recognition": {
      "total": 18,
      "correct": 7,
      "accuracy": 0.3888888888888889
    },
    "Object Reasoning": {
      "total": 27,
      "correct": 9,
      "accuracy": 0.3333333333333333
    },
    "OCR Problems": {
      "total": 4,
      "correct": 4,
      "accuracy": 1.0
    },
    "Temporal Perception": {
      "total": 2,
      "correct": 1,
      "accuracy": 0.5
    },
    "Attribute Perception": {
      "total": 15,
      "correct": 12,
      "accuracy": 0.8
    },
    "Counting Problem": {
      "total": 16,
      "correct": 7,
      "accuracy": 0.4375
    }
  },
  "duration_stats": {
    "short": {
      "total": 60,
      "correct": 39,
      "accuracy": 0.65,
      "avg_retrieval_time": 0.024210973580678304,
      "avg_inference_time": 6.834165279070536,
      "avg_events_used": 4.1
    },
    "medium": {
      "total": 60,
      "correct": 36,
      "accuracy": 0.6,
      "avg_retrieval_time": 0.024613742033640543,
      "avg_inference_time": 10.736228160063426,
      "avg_events_used": 4.85
    },
    "long": {
      "total": 30,
      "correct": 14,
      "accuracy": 0.4666666666666667,
      "avg_retrieval_time": 0.0293193260828654,
      "avg_inference_time": 26.516080101331074,
      "avg_events_used": 5.0
    }
  },
  "triview_stats": {
    "avg_events_per_question": 4.58,
    "avg_top_rrf_score": 0.09483909375871696,
    "avg_mean_rrf_score": 0.08989286315945168,
    "modality_similarities": {
      "visual": {
        "avg_top_similarity": 0.29482099215189617,
        "avg_mean_similarity": 0.28660478760798774
      },
      "semantic": {
        "avg_top_similarity": 0.48130218664805097,
        "avg_mean_similarity": 0.44138418023784953
      },
      "entity": {
        "avg_top_similarity": 0.3872857918341955,
        "avg_mean_similarity": 0.360132498840491
      }
    }
  },
  "results": [
    {
      "video_id": "058",
      "question_id": "058-1",
      "task_type": "Spatial Reasoning",
      "duration": "short",
      "question": "Based on the information provided by the video, which one is the most direct cause of the phenomenon that the smoke flows towards the lamp?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.28147244453430176,
      "inference_time": 8.170691728591919,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09322993420241996,
      "visual_top_sim": 0.27924013137817383,
      "visual_mean_sim": 0.2436581552028656,
      "semantic_top_sim": 0.34261560440063477,
      "semantic_mean_sim": 0.27573367953300476,
      "entity_top_sim": 0.2844150960445404,
      "entity_mean_sim": 0.2112446278333664
    },
    {
      "video_id": "058",
      "question_id": "058-2",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Which color are the mountains in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.022537946701049805,
      "inference_time": 7.015032052993774,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09238790041920972,
      "visual_top_sim": 0.26637253165245056,
      "visual_mean_sim": 0.23880691826343536,
      "semantic_top_sim": 0.6238765716552734,
      "semantic_mean_sim": 0.43639904260635376,
      "entity_top_sim": 0.5190500020980835,
      "entity_mean_sim": 0.4211045205593109
    },
    {
      "video_id": "058",
      "question_id": "058-3",
      "task_type": "Spatial Reasoning",
      "duration": "short",
      "question": "What is one of the causes of wind according to the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.020012378692626953,
      "inference_time": 7.0760109424591064,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09326059798308375,
      "visual_top_sim": 0.2869461476802826,
      "visual_mean_sim": 0.2798353433609009,
      "semantic_top_sim": 0.43035686016082764,
      "semantic_mean_sim": 0.32014551758766174,
      "entity_top_sim": 0.4105713963508606,
      "entity_mean_sim": 0.2911399304866791
    },
    {
      "video_id": "013",
      "question_id": "013-1",
      "task_type": "Spatial Perception",
      "duration": "short",
      "question": "Which of the following is visible in the background of the video when the miniature bottle is shown empty?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.020038127899169922,
      "inference_time": 6.847234010696411,
      "num_events": 5,
      "top_rrf_score": 0.0967741935483871,
      "mean_rrf_score": 0.09339800142931072,
      "visual_top_sim": 0.31755706667900085,
      "visual_mean_sim": 0.3125096261501312,
      "semantic_top_sim": 0.6528035998344421,
      "semantic_mean_sim": 0.6187800168991089,
      "entity_top_sim": 0.5078964233398438,
      "entity_mean_sim": 0.4973069131374359
    },
    {
      "video_id": "013",
      "question_id": "013-2",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "According to the video, which of the following ingredients is not used in the artwork?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019556283950805664,
      "inference_time": 6.621715545654297,
      "num_events": 5,
      "top_rrf_score": 0.09523809523809523,
      "mean_rrf_score": 0.0923731206439935,
      "visual_top_sim": 0.2751767337322235,
      "visual_mean_sim": 0.28246283531188965,
      "semantic_top_sim": 0.3795732855796814,
      "semantic_mean_sim": 0.33880653977394104,
      "entity_top_sim": 0.3358580768108368,
      "entity_mean_sim": 0.2900523543357849
    },
    {
      "video_id": "013",
      "question_id": "013-3",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What was the next step taken by the operator after filling the bottle with oil?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.019800424575805664,
      "inference_time": 7.049180507659912,
      "num_events": 5,
      "top_rrf_score": 0.09684139784946236,
      "mean_rrf_score": 0.09319638852608493,
      "visual_top_sim": 0.2879849076271057,
      "visual_mean_sim": 0.28529709577560425,
      "semantic_top_sim": 0.39212357997894287,
      "semantic_mean_sim": 0.41618871688842773,
      "entity_top_sim": 0.36802083253860474,
      "entity_mean_sim": 0.34532085061073303
    },
    {
      "video_id": "141",
      "question_id": "141-1",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which player does the video call for to finally put the ball in the basket?",
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.019025802612304688,
      "inference_time": 3.306345224380493,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.28762075304985046,
      "visual_mean_sim": 0.28762075304985046,
      "semantic_top_sim": 0.5767742991447449,
      "semantic_mean_sim": 0.5767742991447449,
      "entity_top_sim": 0.592326819896698,
      "entity_mean_sim": 0.592326819896698
    },
    {
      "video_id": "141",
      "question_id": "141-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which player is running the pick-and-roll for the other players?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.01833939552307129,
      "inference_time": 3.3128247261047363,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.2619972825050354,
      "visual_mean_sim": 0.2619972825050354,
      "semantic_top_sim": 0.40891700983047485,
      "semantic_mean_sim": 0.40891700983047485,
      "entity_top_sim": 0.465076208114624,
      "entity_mean_sim": 0.465076208114624
    },
    {
      "video_id": "141",
      "question_id": "141-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which sport's tactics are shown in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.01815199851989746,
      "inference_time": 3.308950185775757,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.22773690521717072,
      "visual_mean_sim": 0.22773690521717072,
      "semantic_top_sim": 0.45845791697502136,
      "semantic_mean_sim": 0.45845791697502136,
      "entity_top_sim": 0.437779039144516,
      "entity_mean_sim": 0.437779039144516
    },
    {
      "video_id": "126",
      "question_id": "126-1",
      "task_type": "Spatial Reasoning",
      "duration": "short",
      "question": "What is the atmosphere portrayed in the video like?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.018823623657226562,
      "inference_time": 6.6042327880859375,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.2537439167499542,
      "visual_mean_sim": 0.2546529471874237,
      "semantic_top_sim": 0.4476316273212433,
      "semantic_mean_sim": 0.3732687830924988,
      "entity_top_sim": 0.30929404497146606,
      "entity_mean_sim": 0.254965215921402
    },
    {
      "video_id": "126",
      "question_id": "126-2",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "What is the reason for entering cold water in the video during the winter season?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.02371835708618164,
      "inference_time": 6.623492956161499,
      "num_events": 5,
      "top_rrf_score": 0.09696969696969696,
      "mean_rrf_score": 0.09393371571502503,
      "visual_top_sim": 0.28302711248397827,
      "visual_mean_sim": 0.28921443223953247,
      "semantic_top_sim": 0.5550093054771423,
      "semantic_mean_sim": 0.4386045038700104,
      "entity_top_sim": 0.5471310615539551,
      "entity_mean_sim": 0.3492681384086609
    },
    {
      "video_id": "126",
      "question_id": "126-3",
      "task_type": "OCR Problems",
      "duration": "short",
      "question": "What is the approximate temperature shown in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.019209623336791992,
      "inference_time": 6.616954565048218,
      "num_events": 5,
      "top_rrf_score": 0.09791666666666665,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.2868919372558594,
      "visual_mean_sim": 0.2885351777076721,
      "semantic_top_sim": 0.5196050405502319,
      "semantic_mean_sim": 0.4004186689853668,
      "entity_top_sim": 0.3709542751312256,
      "entity_mean_sim": 0.2520310580730438
    },
    {
      "video_id": "115",
      "question_id": "115-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "What is the color pattern of the cat in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.01992511749267578,
      "inference_time": 7.4628846645355225,
      "num_events": 5,
      "top_rrf_score": 0.09696969696969696,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.31084930896759033,
      "visual_mean_sim": 0.3027208745479584,
      "semantic_top_sim": 0.663059413433075,
      "semantic_mean_sim": 0.6492632031440735,
      "entity_top_sim": 0.4263293147087097,
      "entity_mean_sim": 0.3897358179092407
    },
    {
      "video_id": "115",
      "question_id": "115-2",
      "task_type": "OCR Problems",
      "duration": "short",
      "question": "According to the video, what is the age range of the cat in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.023961544036865234,
      "inference_time": 7.3248536586761475,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.3024015724658966,
      "visual_mean_sim": 0.2947452664375305,
      "semantic_top_sim": 0.5011582970619202,
      "semantic_mean_sim": 0.4562492370605469,
      "entity_top_sim": 0.25736457109451294,
      "entity_mean_sim": 0.24077653884887695
    },
    {
      "video_id": "115",
      "question_id": "115-3",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many streams did the cat cross in the video?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.019539594650268555,
      "inference_time": 7.3331544399261475,
      "num_events": 5,
      "top_rrf_score": 0.09696969696969696,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.27660876512527466,
      "visual_mean_sim": 0.278849720954895,
      "semantic_top_sim": 0.589444637298584,
      "semantic_mean_sim": 0.5194696187973022,
      "entity_top_sim": 0.39041686058044434,
      "entity_mean_sim": 0.3484930992126465
    },
    {
      "video_id": "072",
      "question_id": "072-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which item does not the man wear in this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019510269165039062,
      "inference_time": 7.168857097625732,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09393371571502501,
      "visual_top_sim": 0.25432834029197693,
      "visual_mean_sim": 0.23509788513183594,
      "semantic_top_sim": 0.47231441736221313,
      "semantic_mean_sim": 0.38365259766578674,
      "entity_top_sim": 0.4102364778518677,
      "entity_mean_sim": 0.31949546933174133
    },
    {
      "video_id": "072",
      "question_id": "072-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "According to the video, what is the right pose demonstrated in the animation when sitting on the bike before starting to ride?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.020099163055419922,
      "inference_time": 7.185894727706909,
      "num_events": 5,
      "top_rrf_score": 0.09684139784946236,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.28318554162979126,
      "visual_mean_sim": 0.27260592579841614,
      "semantic_top_sim": 0.586081326007843,
      "semantic_mean_sim": 0.5453356504440308,
      "entity_top_sim": 0.43552881479263306,
      "entity_mean_sim": 0.32236090302467346
    },
    {
      "video_id": "072",
      "question_id": "072-3",
      "task_type": "Temporal Reasoning",
      "duration": "short",
      "question": "What is the sequence of steps introduced in this video?\n(a) Walk and practice using the brakes.\n(b) Glide to practice balancing.\n(c) Find a good space and safety considerations.\n(d) Start to pedal.\n(e) Adjust the seat.",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.019989728927612305,
      "inference_time": 7.632465124130249,
      "num_events": 5,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.2931814193725586,
      "visual_mean_sim": 0.2738192677497864,
      "semantic_top_sim": 0.39534956216812134,
      "semantic_mean_sim": 0.41968172788619995,
      "entity_top_sim": 0.36299610137939453,
      "entity_mean_sim": 0.2927521765232086
    },
    {
      "video_id": "053",
      "question_id": "053-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "How does the color of lava change when exposed to the air for a short while?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.0201871395111084,
      "inference_time": 7.183147430419922,
      "num_events": 5,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.09280947016353489,
      "visual_top_sim": 0.3642880320549011,
      "visual_mean_sim": 0.35092058777809143,
      "semantic_top_sim": 0.5181084871292114,
      "semantic_mean_sim": 0.47048869729042053,
      "entity_top_sim": 0.43274879455566406,
      "entity_mean_sim": 0.396428644657135
    },
    {
      "video_id": "053",
      "question_id": "053-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "According to the video, how do the geologists collect lava safely?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.02040886878967285,
      "inference_time": 7.151921272277832,
      "num_events": 5,
      "top_rrf_score": 0.09315476190476189,
      "mean_rrf_score": 0.09140760525123354,
      "visual_top_sim": 0.3825972378253937,
      "visual_mean_sim": 0.37215059995651245,
      "semantic_top_sim": 0.7238584756851196,
      "semantic_mean_sim": 0.6771801710128784,
      "entity_top_sim": 0.4543995261192322,
      "entity_mean_sim": 0.4386025369167328
    },
    {
      "video_id": "053",
      "question_id": "053-3",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "How do the geologists cool the lava down?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.019899368286132812,
      "inference_time": 6.961162328720093,
      "num_events": 5,
      "top_rrf_score": 0.09416282642089094,
      "mean_rrf_score": 0.09194876224400342,
      "visual_top_sim": 0.3395633399486542,
      "visual_mean_sim": 0.3512713313102722,
      "semantic_top_sim": 0.5620496869087219,
      "semantic_mean_sim": 0.5413227081298828,
      "entity_top_sim": 0.5262514352798462,
      "entity_mean_sim": 0.46837180852890015
    },
    {
      "video_id": "280",
      "question_id": "280-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which kind of pets is not introduced in this video?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.01926708221435547,
      "inference_time": 7.120835781097412,
      "num_events": 5,
      "top_rrf_score": 0.09481915933528837,
      "mean_rrf_score": 0.092504387226873,
      "visual_top_sim": 0.32407379150390625,
      "visual_mean_sim": 0.31423264741897583,
      "semantic_top_sim": 0.5906156301498413,
      "semantic_mean_sim": 0.5619769096374512,
      "entity_top_sim": 0.34982216358184814,
      "entity_mean_sim": 0.352541983127594
    },
    {
      "video_id": "280",
      "question_id": "280-2",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "How many people are in the family that is playing with a dog at the end of the video?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.01974773406982422,
      "inference_time": 7.311216115951538,
      "num_events": 5,
      "top_rrf_score": 0.09791666666666665,
      "mean_rrf_score": 0.0934193281418139,
      "visual_top_sim": 0.3237455189228058,
      "visual_mean_sim": 0.30732131004333496,
      "semantic_top_sim": 0.5747827291488647,
      "semantic_mean_sim": 0.47428804636001587,
      "entity_top_sim": 0.3343566656112671,
      "entity_mean_sim": 0.3080978989601135
    },
    {
      "video_id": "280",
      "question_id": "280-3",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Which color of fish is absent from the video?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.01912832260131836,
      "inference_time": 6.013689041137695,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.0923082170307028,
      "visual_top_sim": 0.342327356338501,
      "visual_mean_sim": 0.29491758346557617,
      "semantic_top_sim": 0.5954084992408752,
      "semantic_mean_sim": 0.4780298173427582,
      "entity_top_sim": 0.4527793228626251,
      "entity_mean_sim": 0.40064162015914917
    },
    {
      "video_id": "045",
      "question_id": "045-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "What astronomical phenomenon is depicted in the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.02297067642211914,
      "inference_time": 7.174344539642334,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09393371571502501,
      "visual_top_sim": 0.32316064834594727,
      "visual_mean_sim": 0.32146841287612915,
      "semantic_top_sim": 0.6960657835006714,
      "semantic_mean_sim": 0.6436136364936829,
      "entity_top_sim": 0.45428216457366943,
      "entity_mean_sim": 0.44979462027549744
    },
    {
      "video_id": "045",
      "question_id": "045-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which of the following is NOT known to occur during a total solar eclipse, according to the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.01964712142944336,
      "inference_time": 7.221431493759155,
      "num_events": 5,
      "top_rrf_score": 0.09791666666666667,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.3335375189781189,
      "visual_mean_sim": 0.329061359167099,
      "semantic_top_sim": 0.6354180574417114,
      "semantic_mean_sim": 0.5421770811080933,
      "entity_top_sim": 0.32730937004089355,
      "entity_mean_sim": 0.3348352611064911
    },
    {
      "video_id": "045",
      "question_id": "045-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Where were the people in the video located while observing the total solar eclipse?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019744157791137695,
      "inference_time": 7.21288537979126,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09376564848813426,
      "visual_top_sim": 0.3429887592792511,
      "visual_mean_sim": 0.31818699836730957,
      "semantic_top_sim": 0.7357451319694519,
      "semantic_mean_sim": 0.6120550632476807,
      "entity_top_sim": 0.4429475665092468,
      "entity_mean_sim": 0.3703377842903137
    },
    {
      "video_id": "217",
      "question_id": "217-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "What is the physique of the athletes?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.01886916160583496,
      "inference_time": 7.207259654998779,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.27435392141342163,
      "visual_mean_sim": 0.27435392141342163,
      "semantic_top_sim": 0.31060659885406494,
      "semantic_mean_sim": 0.31060659885406494,
      "entity_top_sim": 0.4080207347869873,
      "entity_mean_sim": 0.4080207347869873
    },
    {
      "video_id": "217",
      "question_id": "217-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which sports in this video involve athletes utilizing humans as tools?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.01923203468322754,
      "inference_time": 7.010516881942749,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.28177839517593384,
      "visual_mean_sim": 0.28177839517593384,
      "semantic_top_sim": 0.48532915115356445,
      "semantic_mean_sim": 0.48532915115356445,
      "entity_top_sim": 0.46782878041267395,
      "entity_mean_sim": 0.46782878041267395
    },
    {
      "video_id": "217",
      "question_id": "217-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "What do the athletes wear during their performance?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.019339799880981445,
      "inference_time": 7.026332855224609,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.293813019990921,
      "visual_mean_sim": 0.293813019990921,
      "semantic_top_sim": 0.5325914621353149,
      "semantic_mean_sim": 0.5325914621353149,
      "entity_top_sim": 0.5029432773590088,
      "entity_mean_sim": 0.5029432773590088
    },
    {
      "video_id": "017",
      "question_id": "017-1",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "Based on the information provided by the video, what is the maximum number of fingers required to play this piece of music?",
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.018675565719604492,
      "inference_time": 7.3506903648376465,
      "num_events": 4,
      "top_rrf_score": 0.09791666666666667,
      "mean_rrf_score": 0.09535832111436951,
      "visual_top_sim": 0.3205775022506714,
      "visual_mean_sim": 0.3048066198825836,
      "semantic_top_sim": 0.3152868449687958,
      "semantic_mean_sim": 0.27721190452575684,
      "entity_top_sim": 0.21070599555969238,
      "entity_mean_sim": 0.23071777820587158
    },
    {
      "video_id": "017",
      "question_id": "017-2",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "What do the characters in this video mean?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.019053220748901367,
      "inference_time": 7.312646389007568,
      "num_events": 4,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.09535832111436951,
      "visual_top_sim": 0.2639106810092926,
      "visual_mean_sim": 0.25429093837738037,
      "semantic_top_sim": 0.3880801200866699,
      "semantic_mean_sim": 0.3410811424255371,
      "entity_top_sim": 0.3050451874732971,
      "entity_mean_sim": 0.30099427700042725
    },
    {
      "video_id": "017",
      "question_id": "017-3",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "In this video, why some keys are blue while others are green?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019255876541137695,
      "inference_time": 7.514161586761475,
      "num_events": 4,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.09535832111436951,
      "visual_top_sim": 0.32151320576667786,
      "visual_mean_sim": 0.30743148922920227,
      "semantic_top_sim": 0.5853555202484131,
      "semantic_mean_sim": 0.43746834993362427,
      "entity_top_sim": 0.6169105768203735,
      "entity_mean_sim": 0.40127602219581604
    },
    {
      "video_id": "016",
      "question_id": "016-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "When was the painting mentioned in the video painted?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.02064228057861328,
      "inference_time": 4.825163841247559,
      "num_events": 5,
      "top_rrf_score": 0.0931547619047619,
      "mean_rrf_score": 0.09027687419053643,
      "visual_top_sim": 0.3296952545642853,
      "visual_mean_sim": 0.3397141695022583,
      "semantic_top_sim": 0.6164505481719971,
      "semantic_mean_sim": 0.5647242665290833,
      "entity_top_sim": 0.41309690475463867,
      "entity_mean_sim": 0.4304903447628021
    },
    {
      "video_id": "016",
      "question_id": "016-2",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "Which of the following elements does not appear in the paintings primarily mentioned in the video?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.019505977630615234,
      "inference_time": 4.798698663711548,
      "num_events": 5,
      "top_rrf_score": 0.09096479500891266,
      "mean_rrf_score": 0.08866629563799464,
      "visual_top_sim": 0.31690192222595215,
      "visual_mean_sim": 0.30941009521484375,
      "semantic_top_sim": 0.49058908224105835,
      "semantic_mean_sim": 0.4940873086452484,
      "entity_top_sim": 0.3962634801864624,
      "entity_mean_sim": 0.39163124561309814
    },
    {
      "video_id": "016",
      "question_id": "016-3",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "Which of the following elements is not present in the \"Starry Sky\"?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.019970417022705078,
      "inference_time": 4.774986743927002,
      "num_events": 5,
      "top_rrf_score": 0.09488636363636363,
      "mean_rrf_score": 0.09201822028909315,
      "visual_top_sim": 0.30003470182418823,
      "visual_mean_sim": 0.27726417779922485,
      "semantic_top_sim": 0.4140135645866394,
      "semantic_mean_sim": 0.4257604479789734,
      "entity_top_sim": 0.3536236882209778,
      "entity_mean_sim": 0.3295315206050873
    },
    {
      "video_id": "048",
      "question_id": "048-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which galaxies are depicted in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.0197141170501709,
      "inference_time": 7.509551525115967,
      "num_events": 5,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.09376564848813425,
      "visual_top_sim": 0.3189367651939392,
      "visual_mean_sim": 0.3230530321598053,
      "semantic_top_sim": 0.7042368650436401,
      "semantic_mean_sim": 0.6401599645614624,
      "entity_top_sim": 0.5859757661819458,
      "entity_mean_sim": 0.5081357955932617
    },
    {
      "video_id": "048",
      "question_id": "048-2",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "What significant event is the number at the bottom of the video referring to?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.0193328857421875,
      "inference_time": 7.548988342285156,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.09325126091492315,
      "visual_top_sim": 0.30565160512924194,
      "visual_mean_sim": 0.28789782524108887,
      "semantic_top_sim": 0.3982160687446594,
      "semantic_mean_sim": 0.367469847202301,
      "entity_top_sim": 0.3097715377807617,
      "entity_mean_sim": 0.2669054865837097
    },
    {
      "video_id": "048",
      "question_id": "048-3",
      "task_type": "Object Reasoning",
      "duration": "short",
      "question": "Which of the following statements can be inferred about the Triangulum Galaxy (M33) based on the information presented in the video about the other two galaxies?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.019986629486083984,
      "inference_time": 7.723111391067505,
      "num_events": 5,
      "top_rrf_score": 0.09684139784946236,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.31338098645210266,
      "visual_mean_sim": 0.3069271147251129,
      "semantic_top_sim": 0.5482943654060364,
      "semantic_mean_sim": 0.5175561904907227,
      "entity_top_sim": 0.3655167818069458,
      "entity_mean_sim": 0.38935166597366333
    },
    {
      "video_id": "112",
      "question_id": "112-1",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many birds can be observed perched on the street lamp in the first half of the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.023502588272094727,
      "inference_time": 6.07636022567749,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09129217544540125,
      "visual_top_sim": 0.3057146668434143,
      "visual_mean_sim": 0.27384886145591736,
      "semantic_top_sim": 0.5646223425865173,
      "semantic_mean_sim": 0.4077379107475281,
      "entity_top_sim": 0.3592216372489929,
      "entity_mean_sim": 0.32423314452171326
    },
    {
      "video_id": "112",
      "question_id": "112-2",
      "task_type": "Information Synopsis",
      "duration": "short",
      "question": "What do the people in the village show in the video used to illuminate the night?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.02018570899963379,
      "inference_time": 6.055006504058838,
      "num_events": 5,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.09262007622414094,
      "visual_top_sim": 0.3257664740085602,
      "visual_mean_sim": 0.2991449236869812,
      "semantic_top_sim": 0.673120379447937,
      "semantic_mean_sim": 0.5759183168411255,
      "entity_top_sim": 0.47320830821990967,
      "entity_mean_sim": 0.42763271927833557
    },
    {
      "video_id": "112",
      "question_id": "112-3",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "What is the purpose of collecting sap from a rubber tree in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.019420623779296875,
      "inference_time": 6.9521565437316895,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.0927836504473127,
      "visual_top_sim": 0.3498903512954712,
      "visual_mean_sim": 0.32101351022720337,
      "semantic_top_sim": 0.5135427713394165,
      "semantic_mean_sim": 0.3920029103755951,
      "entity_top_sim": 0.29570460319519043,
      "entity_mean_sim": 0.245608851313591
    },
    {
      "video_id": "120",
      "question_id": "120-1",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "What is the animal in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.01924872398376465,
      "inference_time": 6.795753479003906,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.30093809962272644,
      "visual_mean_sim": 0.30093809962272644,
      "semantic_top_sim": 0.5534529089927673,
      "semantic_mean_sim": 0.5534529089927673,
      "entity_top_sim": 0.4077075719833374,
      "entity_mean_sim": 0.4077075719833374
    },
    {
      "video_id": "120",
      "question_id": "120-2",
      "task_type": "Action Reasoning",
      "duration": "short",
      "question": "What is the reason for the lamp's action in the video, slapping the human portrayed?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.01957535743713379,
      "inference_time": 6.830844163894653,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.26766103506088257,
      "visual_mean_sim": 0.26766103506088257,
      "semantic_top_sim": 0.4131496548652649,
      "semantic_mean_sim": 0.4131496548652649,
      "entity_top_sim": 0.21248653531074524,
      "entity_mean_sim": 0.21248653531074524
    },
    {
      "video_id": "120",
      "question_id": "120-3",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Which color of clothing is the person wearing in the video?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.01769852638244629,
      "inference_time": 6.970763683319092,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.23210389912128448,
      "visual_mean_sim": 0.23210389912128448,
      "semantic_top_sim": 0.49306491017341614,
      "semantic_mean_sim": 0.49306491017341614,
      "entity_top_sim": 0.4225331246852875,
      "entity_mean_sim": 0.4225331246852875
    },
    {
      "video_id": "259",
      "question_id": "259-1",
      "task_type": "OCR Problems",
      "duration": "short",
      "question": "What time does the man in the video get up?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.018984079360961914,
      "inference_time": 7.029559135437012,
      "num_events": 5,
      "top_rrf_score": 0.09607843137254901,
      "mean_rrf_score": 0.0930712040436898,
      "visual_top_sim": 0.2896638810634613,
      "visual_mean_sim": 0.29993125796318054,
      "semantic_top_sim": 0.6115336418151855,
      "semantic_mean_sim": 0.48829954862594604,
      "entity_top_sim": 0.39974600076675415,
      "entity_mean_sim": 0.3210156559944153
    },
    {
      "video_id": "259",
      "question_id": "259-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "Which activity is not shown in the video?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.019283771514892578,
      "inference_time": 7.249773979187012,
      "num_events": 5,
      "top_rrf_score": 0.09576612903225806,
      "mean_rrf_score": 0.09221439048526334,
      "visual_top_sim": 0.2799769639968872,
      "visual_mean_sim": 0.2806779742240906,
      "semantic_top_sim": 0.40198278427124023,
      "semantic_mean_sim": 0.37507015466690063,
      "entity_top_sim": 0.3498132824897766,
      "entity_mean_sim": 0.32314926385879517
    },
    {
      "video_id": "259",
      "question_id": "259-3",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What does the man shown in the video do after swimming?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.019426584243774414,
      "inference_time": 7.008345365524292,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09147630771195137,
      "visual_top_sim": 0.28091228008270264,
      "visual_mean_sim": 0.27187275886535645,
      "semantic_top_sim": 0.43806731700897217,
      "semantic_mean_sim": 0.4360807538032532,
      "entity_top_sim": 0.31031060218811035,
      "entity_mean_sim": 0.31083765625953674
    },
    {
      "video_id": "014",
      "question_id": "014-1",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Which of the following can be identified as Susan White-Oakes' most recent art piece, based on the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.01972174644470215,
      "inference_time": 7.527087450027466,
      "num_events": 4,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.09535832111436951,
      "visual_top_sim": 0.2785787582397461,
      "visual_mean_sim": 0.25814521312713623,
      "semantic_top_sim": 0.5568712949752808,
      "semantic_mean_sim": 0.43214160203933716,
      "entity_top_sim": 0.29102832078933716,
      "entity_mean_sim": 0.3006599247455597
    },
    {
      "video_id": "014",
      "question_id": "014-2",
      "task_type": "Attribute Perception",
      "duration": "short",
      "question": "Based on the video, what is Susan White-Oakes' latest art work made of?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.01910567283630371,
      "inference_time": 7.699620246887207,
      "num_events": 4,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.0953583211143695,
      "visual_top_sim": 0.29074832797050476,
      "visual_mean_sim": 0.26673343777656555,
      "semantic_top_sim": 0.6013205647468567,
      "semantic_mean_sim": 0.459196001291275,
      "entity_top_sim": 0.35821181535720825,
      "entity_mean_sim": 0.3472440242767334
    },
    {
      "video_id": "014",
      "question_id": "014-3",
      "task_type": "Temporal Reasoning",
      "duration": "short",
      "question": "What does Susan White-Oakes often do before polishing the whole body?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.01933908462524414,
      "inference_time": 7.536448001861572,
      "num_events": 4,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09535832111436951,
      "visual_top_sim": 0.28868088126182556,
      "visual_mean_sim": 0.26187393069267273,
      "semantic_top_sim": 0.35896801948547363,
      "semantic_mean_sim": 0.24619585275650024,
      "entity_top_sim": 0.31580811738967896,
      "entity_mean_sim": 0.2737049162387848
    },
    {
      "video_id": "288",
      "question_id": "288-1",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What were the movements of the individuals in the video during the competition process?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.019083738327026367,
      "inference_time": 7.188366413116455,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.25796687602996826,
      "visual_mean_sim": 0.25796687602996826,
      "semantic_top_sim": 0.5033615827560425,
      "semantic_mean_sim": 0.5033615827560425,
      "entity_top_sim": 0.3888883590698242,
      "entity_mean_sim": 0.3888883590698242
    },
    {
      "video_id": "288",
      "question_id": "288-2",
      "task_type": "Spatial Perception",
      "duration": "short",
      "question": "Where does the participant need to place his/her hand during the game in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019174575805664062,
      "inference_time": 7.20682954788208,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.26198920607566833,
      "visual_mean_sim": 0.26198920607566833,
      "semantic_top_sim": 0.4069950580596924,
      "semantic_mean_sim": 0.4069950580596924,
      "entity_top_sim": 0.38735803961753845,
      "entity_mean_sim": 0.38735803961753845
    },
    {
      "video_id": "288",
      "question_id": "288-3",
      "task_type": "Object Recognition",
      "duration": "short",
      "question": "Who is the last to reach the finish line at the end of the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.01944279670715332,
      "inference_time": 7.200349807739258,
      "num_events": 1,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.1,
      "visual_top_sim": 0.2631714344024658,
      "visual_mean_sim": 0.2631714344024658,
      "semantic_top_sim": 0.3007417619228363,
      "semantic_mean_sim": 0.3007417619228363,
      "entity_top_sim": 0.3192368745803833,
      "entity_mean_sim": 0.3192368745803833
    },
    {
      "video_id": "215",
      "question_id": "215-1",
      "task_type": "Information Synopsis",
      "duration": "short",
      "question": "What is this video mainly about?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.019404172897338867,
      "inference_time": 7.655129909515381,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09325126091492315,
      "visual_top_sim": 0.270477294921875,
      "visual_mean_sim": 0.2549206614494324,
      "semantic_top_sim": 0.5712766051292419,
      "semantic_mean_sim": 0.45935168862342834,
      "entity_top_sim": 0.38055241107940674,
      "entity_mean_sim": 0.31186771392822266
    },
    {
      "video_id": "215",
      "question_id": "215-2",
      "task_type": "Action Recognition",
      "duration": "short",
      "question": "What is the ending pose of the team at the end of this video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.019617557525634766,
      "inference_time": 7.491599082946777,
      "num_events": 5,
      "top_rrf_score": 0.09684139784946236,
      "mean_rrf_score": 0.09238790041920972,
      "visual_top_sim": 0.32943642139434814,
      "visual_mean_sim": 0.31536665558815,
      "semantic_top_sim": 0.47933632135391235,
      "semantic_mean_sim": 0.4638376832008362,
      "entity_top_sim": 0.40911826491355896,
      "entity_mean_sim": 0.4020155072212219
    },
    {
      "video_id": "215",
      "question_id": "215-3",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many individuals are in the team, with each person dressed in yellow?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.019664764404296875,
      "inference_time": 7.481531143188477,
      "num_events": 5,
      "top_rrf_score": 0.1,
      "mean_rrf_score": 0.09274621040987265,
      "visual_top_sim": 0.24893680214881897,
      "visual_mean_sim": 0.23517823219299316,
      "semantic_top_sim": 0.3749568462371826,
      "semantic_mean_sim": 0.32240211963653564,
      "entity_top_sim": 0.48947352170944214,
      "entity_mean_sim": 0.42766889929771423
    },
    {
      "video_id": "413",
      "question_id": "413-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What is \"Avenues for Justice\" mentioned in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.02124643325805664,
      "inference_time": 12.364300966262817,
      "num_events": 5,
      "top_rrf_score": 0.09053509154315606,
      "mean_rrf_score": 0.0871451498703571,
      "visual_top_sim": 0.26851990818977356,
      "visual_mean_sim": 0.27767878770828247,
      "semantic_top_sim": 0.38876649737358093,
      "semantic_mean_sim": 0.36328965425491333,
      "entity_top_sim": 0.29684412479400635,
      "entity_mean_sim": 0.28410089015960693
    },
    {
      "video_id": "413",
      "question_id": "413-2",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is this video about?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.021579265594482422,
      "inference_time": 12.3362717628479,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.08479113587995261,
      "visual_top_sim": 0.29924649000167847,
      "visual_mean_sim": 0.28858980536460876,
      "semantic_top_sim": 0.542793869972229,
      "semantic_mean_sim": 0.505089282989502,
      "entity_top_sim": 0.3822648227214813,
      "entity_mean_sim": 0.3146105706691742
    },
    {
      "video_id": "413",
      "question_id": "413-3",
      "task_type": "Temporal Perception",
      "duration": "medium",
      "question": "In which part of the video is the woman in the blue top interviewed?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.021790504455566406,
      "inference_time": 12.490063905715942,
      "num_events": 5,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.09035153191763319,
      "visual_top_sim": 0.2800667881965637,
      "visual_mean_sim": 0.27674224972724915,
      "semantic_top_sim": 0.5992007255554199,
      "semantic_mean_sim": 0.5525871515274048,
      "entity_top_sim": 0.3206687569618225,
      "entity_mean_sim": 0.28952866792678833
    },
    {
      "video_id": "530",
      "question_id": "530-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which of the following materials is not used in this video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.021782875061035156,
      "inference_time": 10.39240026473999,
      "num_events": 5,
      "top_rrf_score": 0.09336917562724015,
      "mean_rrf_score": 0.09005673390036219,
      "visual_top_sim": 0.27949756383895874,
      "visual_mean_sim": 0.28006964921951294,
      "semantic_top_sim": 0.37851208448410034,
      "semantic_mean_sim": 0.3667132258415222,
      "entity_top_sim": 0.3647690415382385,
      "entity_mean_sim": 0.33396288752555847
    },
    {
      "video_id": "530",
      "question_id": "530-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many stars can be extracted from one CD?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.019413232803344727,
      "inference_time": 10.404309511184692,
      "num_events": 5,
      "top_rrf_score": 0.09696969696969696,
      "mean_rrf_score": 0.09259425650791873,
      "visual_top_sim": 0.2956470847129822,
      "visual_mean_sim": 0.2979546785354614,
      "semantic_top_sim": 0.33670559525489807,
      "semantic_mean_sim": 0.25663942098617554,
      "entity_top_sim": 0.22712108492851257,
      "entity_mean_sim": 0.17179974913597107
    },
    {
      "video_id": "530",
      "question_id": "530-3",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is this video mainly about?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.019814729690551758,
      "inference_time": 10.408594846725464,
      "num_events": 5,
      "top_rrf_score": 0.09304812834224599,
      "mean_rrf_score": 0.09069435545479188,
      "visual_top_sim": 0.2740488648414612,
      "visual_mean_sim": 0.27266040444374084,
      "semantic_top_sim": 0.4074740409851074,
      "semantic_mean_sim": 0.3844783306121826,
      "entity_top_sim": 0.24678590893745422,
      "entity_mean_sim": 0.23771727085113525
    },
    {
      "video_id": "443",
      "question_id": "443-1",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many attempts do it take for Phonzy to hit a 3-pointer in the first game?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.021157503128051758,
      "inference_time": 10.766184568405151,
      "num_events": 5,
      "top_rrf_score": 0.09791666666666665,
      "mean_rrf_score": 0.09197705230674869,
      "visual_top_sim": 0.30667001008987427,
      "visual_mean_sim": 0.2981962561607361,
      "semantic_top_sim": 0.36789944767951965,
      "semantic_mean_sim": 0.3161131739616394,
      "entity_top_sim": 0.43804287910461426,
      "entity_mean_sim": 0.3685198426246643
    },
    {
      "video_id": "443",
      "question_id": "443-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many free throws does Jamal hit in game 2?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.1972193717956543,
      "inference_time": 10.76907753944397,
      "num_events": 5,
      "top_rrf_score": 0.09696969696969696,
      "mean_rrf_score": 0.09070089461455685,
      "visual_top_sim": 0.2787283658981323,
      "visual_mean_sim": 0.2721344828605652,
      "semantic_top_sim": 0.3527325391769409,
      "semantic_mean_sim": 0.3206841051578522,
      "entity_top_sim": 0.43240052461624146,
      "entity_mean_sim": 0.4023999273777008
    },
    {
      "video_id": "443",
      "question_id": "443-3",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "Why is there no winner in Game 3?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.019950151443481445,
      "inference_time": 10.771512031555176,
      "num_events": 5,
      "top_rrf_score": 0.09791666666666665,
      "mean_rrf_score": 0.09235872208120784,
      "visual_top_sim": 0.28515151143074036,
      "visual_mean_sim": 0.27697306871414185,
      "semantic_top_sim": 0.4270002841949463,
      "semantic_mean_sim": 0.26816853880882263,
      "entity_top_sim": 0.4039532542228699,
      "entity_mean_sim": 0.315017431974411
    },
    {
      "video_id": "304",
      "question_id": "304-1",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "Why did the main character in the video put his hand in the coat?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.023275136947631836,
      "inference_time": 9.213234186172485,
      "num_events": 5,
      "top_rrf_score": 0.09500316255534472,
      "mean_rrf_score": 0.08216409659320219,
      "visual_top_sim": 0.3141661584377289,
      "visual_mean_sim": 0.29063862562179565,
      "semantic_top_sim": 0.6070583462715149,
      "semantic_mean_sim": 0.5518776178359985,
      "entity_top_sim": 0.3879217505455017,
      "entity_mean_sim": 0.3948984742164612
    },
    {
      "video_id": "304",
      "question_id": "304-2",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "Who started the custom of restraining hand activities in pulic?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.023840665817260742,
      "inference_time": 10.356330633163452,
      "num_events": 5,
      "top_rrf_score": 0.08499149659863944,
      "mean_rrf_score": 0.07856314177906631,
      "visual_top_sim": 0.24311906099319458,
      "visual_mean_sim": 0.24576488137245178,
      "semantic_top_sim": 0.23090943694114685,
      "semantic_mean_sim": 0.22520537674427032,
      "entity_top_sim": 0.35042983293533325,
      "entity_mean_sim": 0.32505229115486145
    },
    {
      "video_id": "304",
      "question_id": "304-3",
      "task_type": "Spatial Perception",
      "duration": "medium",
      "question": "Which of the following elements isn't mentioned in the painting \"The Emperor Napoleon in His Study at the Tuileries\"?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.026265859603881836,
      "inference_time": 9.195322513580322,
      "num_events": 5,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.08715104816148458,
      "visual_top_sim": 0.34749627113342285,
      "visual_mean_sim": 0.3266989588737488,
      "semantic_top_sim": 0.5293272733688354,
      "semantic_mean_sim": 0.4692518711090088,
      "entity_top_sim": 0.3506621718406677,
      "entity_mean_sim": 0.342673122882843
    },
    {
      "video_id": "382",
      "question_id": "382-1",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "According to the video, why is IMAX not suitable for filming movies?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.021701574325561523,
      "inference_time": 9.575188398361206,
      "num_events": 5,
      "top_rrf_score": 0.09304812834224599,
      "mean_rrf_score": 0.08820808712174937,
      "visual_top_sim": 0.36129030585289,
      "visual_mean_sim": 0.3258597254753113,
      "semantic_top_sim": 0.5677988529205322,
      "semantic_mean_sim": 0.5822778344154358,
      "entity_top_sim": 0.31052321195602417,
      "entity_mean_sim": 0.3231164515018463
    },
    {
      "video_id": "382",
      "question_id": "382-2",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "According to the video, which statement is correct?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.02146148681640625,
      "inference_time": 9.407777070999146,
      "num_events": 5,
      "top_rrf_score": 0.08695133872159835,
      "mean_rrf_score": 0.08434617679341108,
      "visual_top_sim": 0.2759161591529846,
      "visual_mean_sim": 0.27912330627441406,
      "semantic_top_sim": 0.34705236554145813,
      "semantic_mean_sim": 0.31966280937194824,
      "entity_top_sim": 0.2829318046569824,
      "entity_mean_sim": 0.2748125195503235
    },
    {
      "video_id": "382",
      "question_id": "382-3",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which IMAX movie isn't in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.021502017974853516,
      "inference_time": 9.412499904632568,
      "num_events": 5,
      "top_rrf_score": 0.09500316255534472,
      "mean_rrf_score": 0.08988035396005035,
      "visual_top_sim": 0.32981979846954346,
      "visual_mean_sim": 0.3278048634529114,
      "semantic_top_sim": 0.6201932430267334,
      "semantic_mean_sim": 0.5746809840202332,
      "entity_top_sim": 0.39593714475631714,
      "entity_mean_sim": 0.37486445903778076
    },
    {
      "video_id": "517",
      "question_id": "517-1",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many female performers are doing this show?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019141197204589844,
      "inference_time": 12.575151920318604,
      "num_events": 4,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.09535832111436948,
      "visual_top_sim": 0.2649497389793396,
      "visual_mean_sim": 0.2675054669380188,
      "semantic_top_sim": 0.35996267199516296,
      "semantic_mean_sim": 0.33531802892684937,
      "entity_top_sim": 0.3036178648471832,
      "entity_mean_sim": 0.34143778681755066
    },
    {
      "video_id": "517",
      "question_id": "517-2",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "What happened after the two actresses climbed onto the high board?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.019434213638305664,
      "inference_time": 12.748090267181396,
      "num_events": 4,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09535832111436951,
      "visual_top_sim": 0.27810272574424744,
      "visual_mean_sim": 0.26898664236068726,
      "semantic_top_sim": 0.2543424963951111,
      "semantic_mean_sim": 0.22106964886188507,
      "entity_top_sim": 0.3621125817298889,
      "entity_mean_sim": 0.3374195098876953
    },
    {
      "video_id": "517",
      "question_id": "517-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What is the function of the actor hanging on the left swing?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.01936793327331543,
      "inference_time": 12.488712549209595,
      "num_events": 4,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.09535832111436951,
      "visual_top_sim": 0.29617202281951904,
      "visual_mean_sim": 0.29280632734298706,
      "semantic_top_sim": 0.3581252098083496,
      "semantic_mean_sim": 0.31503841280937195,
      "entity_top_sim": 0.4176013469696045,
      "entity_mean_sim": 0.39549973607063293
    },
    {
      "video_id": "475",
      "question_id": "475-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What sport are the two athletes playing?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.01933455467224121,
      "inference_time": 8.888094663619995,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09393371571502501,
      "visual_top_sim": 0.2746981084346771,
      "visual_mean_sim": 0.26682883501052856,
      "semantic_top_sim": 0.4214002788066864,
      "semantic_mean_sim": 0.40017199516296387,
      "entity_top_sim": 0.45400774478912354,
      "entity_mean_sim": 0.4158356785774231
    },
    {
      "video_id": "475",
      "question_id": "475-2",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "How many points does the winner get enough to win the match?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.019319772720336914,
      "inference_time": 8.862449645996094,
      "num_events": 5,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.09393371571502503,
      "visual_top_sim": 0.30263328552246094,
      "visual_mean_sim": 0.2849021255970001,
      "semantic_top_sim": 0.35871607065200806,
      "semantic_mean_sim": 0.2693905234336853,
      "entity_top_sim": 0.3467916250228882,
      "entity_mean_sim": 0.32582828402519226
    },
    {
      "video_id": "475",
      "question_id": "475-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What do the two players have in common?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.019427061080932617,
      "inference_time": 8.931996822357178,
      "num_events": 5,
      "top_rrf_score": 0.0967741935483871,
      "mean_rrf_score": 0.09393371571502503,
      "visual_top_sim": 0.24641187489032745,
      "visual_mean_sim": 0.24214012920856476,
      "semantic_top_sim": 0.41089409589767456,
      "semantic_mean_sim": 0.3788902163505554,
      "entity_top_sim": 0.4640105664730072,
      "entity_mean_sim": 0.42404183745384216
    },
    {
      "video_id": "380",
      "question_id": "380-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "As can be seen in the video, which food is not used to practice holding chopsticks?",
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.022562265396118164,
      "inference_time": 9.747022151947021,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09096437081843553,
      "visual_top_sim": 0.34455400705337524,
      "visual_mean_sim": 0.3316326141357422,
      "semantic_top_sim": 0.5860601663589478,
      "semantic_mean_sim": 0.5858280062675476,
      "entity_top_sim": 0.5513803958892822,
      "entity_mean_sim": 0.4203004240989685
    },
    {
      "video_id": "380",
      "question_id": "380-2",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "As depicted in the video, which finger touches the chopsticks?",
      "predicted": "A",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.019202470779418945,
      "inference_time": 9.55748438835144,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.09105441965848438,
      "visual_top_sim": 0.36468324065208435,
      "visual_mean_sim": 0.3626461625099182,
      "semantic_top_sim": 0.6875481605529785,
      "semantic_mean_sim": 0.6249332427978516,
      "entity_top_sim": 0.6205748319625854,
      "entity_mean_sim": 0.4458068907260895
    },
    {
      "video_id": "380",
      "question_id": "380-3",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is this video mainly about?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.021330833435058594,
      "inference_time": 9.53749942779541,
      "num_events": 5,
      "top_rrf_score": 0.09336917562724015,
      "mean_rrf_score": 0.09058675007316773,
      "visual_top_sim": 0.27093344926834106,
      "visual_mean_sim": 0.2649218440055847,
      "semantic_top_sim": 0.37854522466659546,
      "semantic_mean_sim": 0.4215014576911926,
      "entity_top_sim": 0.2734467089176178,
      "entity_mean_sim": 0.25542908906936646
    },
    {
      "video_id": "411",
      "question_id": "411-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What caused the dilapidated scene at the beginning of the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.022609710693359375,
      "inference_time": 12.445186376571655,
      "num_events": 5,
      "top_rrf_score": 0.0919728595250417,
      "mean_rrf_score": 0.08985327937415225,
      "visual_top_sim": 0.2936360239982605,
      "visual_mean_sim": 0.29607677459716797,
      "semantic_top_sim": 0.5643225908279419,
      "semantic_mean_sim": 0.5340172648429871,
      "entity_top_sim": 0.3809943199157715,
      "entity_mean_sim": 0.3777914047241211
    },
    {
      "video_id": "411",
      "question_id": "411-2",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "What are the children wearing pink outfits doing at the beginning of the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.022420406341552734,
      "inference_time": 12.453300952911377,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.09061300864431794,
      "visual_top_sim": 0.33567970991134644,
      "visual_mean_sim": 0.29763492941856384,
      "semantic_top_sim": 0.5746508836746216,
      "semantic_mean_sim": 0.5671054720878601,
      "entity_top_sim": 0.535419225692749,
      "entity_mean_sim": 0.4673124849796295
    },
    {
      "video_id": "411",
      "question_id": "411-3",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "In the middle of the video, what are the difficulties of rebuilding after the earthquake?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.024373531341552734,
      "inference_time": 12.497614622116089,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09011268165792283,
      "visual_top_sim": 0.3717871904373169,
      "visual_mean_sim": 0.3438296318054199,
      "semantic_top_sim": 0.529180645942688,
      "semantic_mean_sim": 0.4939592778682709,
      "entity_top_sim": 0.45526987314224243,
      "entity_mean_sim": 0.42913714051246643
    },
    {
      "video_id": "473",
      "question_id": "473-1",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many men's single matches are included in this video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.01944255828857422,
      "inference_time": 9.302942991256714,
      "num_events": 5,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.09290313681679904,
      "visual_top_sim": 0.30585676431655884,
      "visual_mean_sim": 0.29532065987586975,
      "semantic_top_sim": 0.3661407232284546,
      "semantic_mean_sim": 0.3448893427848816,
      "entity_top_sim": 0.28047436475753784,
      "entity_mean_sim": 0.2945388853549957
    },
    {
      "video_id": "473",
      "question_id": "473-2",
      "task_type": "Spatial Perception",
      "duration": "medium",
      "question": "Where is the first match held?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.020184040069580078,
      "inference_time": 9.331802129745483,
      "num_events": 5,
      "top_rrf_score": 0.09576612903225806,
      "mean_rrf_score": 0.09376564848813426,
      "visual_top_sim": 0.28382134437561035,
      "visual_mean_sim": 0.27993181347846985,
      "semantic_top_sim": 0.32375550270080566,
      "semantic_mean_sim": 0.3144124150276184,
      "entity_top_sim": 0.34592804312705994,
      "entity_mean_sim": 0.33953386545181274
    },
    {
      "video_id": "473",
      "question_id": "473-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "Which team won the first men's doubles?",
      "predicted": "A",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.01936173439025879,
      "inference_time": 9.279443502426147,
      "num_events": 5,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.09393371571502503,
      "visual_top_sim": 0.28388065099716187,
      "visual_mean_sim": 0.286834180355072,
      "semantic_top_sim": 0.19630268216133118,
      "semantic_mean_sim": 0.18691906332969666,
      "entity_top_sim": 0.3477020263671875,
      "entity_mean_sim": 0.2811182737350464
    },
    {
      "video_id": "353",
      "question_id": "353-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What's the weather like in the city at the beginning of the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.023439884185791016,
      "inference_time": 12.956339836120605,
      "num_events": 5,
      "top_rrf_score": 0.0920794930875576,
      "mean_rrf_score": 0.08608659168817591,
      "visual_top_sim": 0.3031540811061859,
      "visual_mean_sim": 0.2930222153663635,
      "semantic_top_sim": 0.587490975856781,
      "semantic_mean_sim": 0.555949330329895,
      "entity_top_sim": 0.4316454827785492,
      "entity_mean_sim": 0.43859463930130005
    },
    {
      "video_id": "353",
      "question_id": "353-2",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What is the color of the clothes that the speaker is wearing in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.029161453247070312,
      "inference_time": 13.146693468093872,
      "num_events": 5,
      "top_rrf_score": 0.09500316255534472,
      "mean_rrf_score": 0.08353427187137685,
      "visual_top_sim": 0.275694876909256,
      "visual_mean_sim": 0.26374632120132446,
      "semantic_top_sim": 0.4496215283870697,
      "semantic_mean_sim": 0.4378713071346283,
      "entity_top_sim": 0.4227569103240967,
      "entity_mean_sim": 0.3991270363330841
    },
    {
      "video_id": "353",
      "question_id": "353-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What does the speaker explain before introducing MS4?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.02455759048461914,
      "inference_time": 12.959118843078613,
      "num_events": 5,
      "top_rrf_score": 0.09684139784946236,
      "mean_rrf_score": 0.0846937228237575,
      "visual_top_sim": 0.2648037374019623,
      "visual_mean_sim": 0.25596246123313904,
      "semantic_top_sim": 0.2508889436721802,
      "semantic_mean_sim": 0.1865229457616806,
      "entity_top_sim": 0.2533782720565796,
      "entity_mean_sim": 0.23067530989646912
    },
    {
      "video_id": "348",
      "question_id": "348-1",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "Which specific phenomenon during the video demonstrated the correctness of general relativity?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.027317047119140625,
      "inference_time": 9.046950340270996,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.085476915551194,
      "visual_top_sim": 0.31622862815856934,
      "visual_mean_sim": 0.29932230710983276,
      "semantic_top_sim": 0.5846416354179382,
      "semantic_mean_sim": 0.538287341594696,
      "entity_top_sim": 0.39584553241729736,
      "entity_mean_sim": 0.31653279066085815
    },
    {
      "video_id": "348",
      "question_id": "348-2",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is the video about?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.021819114685058594,
      "inference_time": 9.281905889511108,
      "num_events": 5,
      "top_rrf_score": 0.09607843137254901,
      "mean_rrf_score": 0.08388117752436142,
      "visual_top_sim": 0.27488958835601807,
      "visual_mean_sim": 0.26999154686927795,
      "semantic_top_sim": 0.5046001672744751,
      "semantic_mean_sim": 0.4345317482948303,
      "entity_top_sim": 0.32367444038391113,
      "entity_mean_sim": 0.3051360249519348
    },
    {
      "video_id": "348",
      "question_id": "348-3",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "At least how many total solar eclipses occurred between 1921 and 1970 according to the video?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.02656388282775879,
      "inference_time": 9.015010356903076,
      "num_events": 5,
      "top_rrf_score": 0.09607843137254901,
      "mean_rrf_score": 0.09143551946682875,
      "visual_top_sim": 0.36654719710350037,
      "visual_mean_sim": 0.3467022776603699,
      "semantic_top_sim": 0.617023229598999,
      "semantic_mean_sim": 0.6025943756103516,
      "entity_top_sim": 0.29619407653808594,
      "entity_mean_sim": 0.3208945095539093
    },
    {
      "video_id": "495",
      "question_id": "495-1",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "What type of cards does the magician ask the female judge to pick one from?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.022652149200439453,
      "inference_time": 13.02036714553833,
      "num_events": 5,
      "top_rrf_score": 0.09336917562724013,
      "mean_rrf_score": 0.08888014167538286,
      "visual_top_sim": 0.3148127794265747,
      "visual_mean_sim": 0.29669469594955444,
      "semantic_top_sim": 0.6142711639404297,
      "semantic_mean_sim": 0.5082694888114929,
      "entity_top_sim": 0.3874611258506775,
      "entity_mean_sim": 0.4220956861972809
    },
    {
      "video_id": "495",
      "question_id": "495-2",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "What magic does the magician first perform on the stage?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.023886919021606445,
      "inference_time": 12.862187147140503,
      "num_events": 5,
      "top_rrf_score": 0.08977212506624271,
      "mean_rrf_score": 0.08404094792158569,
      "visual_top_sim": 0.31839489936828613,
      "visual_mean_sim": 0.29979175329208374,
      "semantic_top_sim": 0.5932539105415344,
      "semantic_mean_sim": 0.5652368664741516,
      "entity_top_sim": 0.4120640456676483,
      "entity_mean_sim": 0.44130954146385193
    },
    {
      "video_id": "495",
      "question_id": "495-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What do the two performances have in common?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.022963285446166992,
      "inference_time": 12.860219955444336,
      "num_events": 5,
      "top_rrf_score": 0.08914909015715466,
      "mean_rrf_score": 0.08366289088983359,
      "visual_top_sim": 0.2723369002342224,
      "visual_mean_sim": 0.2683691084384918,
      "semantic_top_sim": 0.3676517605781555,
      "semantic_mean_sim": 0.3932895064353943,
      "entity_top_sim": 0.44460636377334595,
      "entity_mean_sim": 0.41534876823425293
    },
    {
      "video_id": "350",
      "question_id": "350-1",
      "task_type": "Information Synopsis",
      "duration": "medium",
      "question": "What is the primary topic of the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.022968292236328125,
      "inference_time": 9.106154203414917,
      "num_events": 5,
      "top_rrf_score": 0.09059139784946237,
      "mean_rrf_score": 0.08196940794219806,
      "visual_top_sim": 0.25531938672065735,
      "visual_mean_sim": 0.26476314663887024,
      "semantic_top_sim": 0.4576038122177124,
      "semantic_mean_sim": 0.4256475865840912,
      "entity_top_sim": 0.39203667640686035,
      "entity_mean_sim": 0.3318735361099243
    },
    {
      "video_id": "350",
      "question_id": "350-2",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "How does the diameter of the black hole S5 0014+81 compare to other distances in space?",
      "predicted": "B",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.023283004760742188,
      "inference_time": 7.965752840042114,
      "num_events": 5,
      "top_rrf_score": 0.09336917562724013,
      "mean_rrf_score": 0.09022324421931152,
      "visual_top_sim": 0.3188626170158386,
      "visual_mean_sim": 0.3036108911037445,
      "semantic_top_sim": 0.4397221803665161,
      "semantic_mean_sim": 0.40383005142211914,
      "entity_top_sim": 0.20649214088916779,
      "entity_mean_sim": 0.219563290476799
    },
    {
      "video_id": "350",
      "question_id": "350-3",
      "task_type": "Object Reasoning",
      "duration": "medium",
      "question": "What indicates if we travel inside a black hole for quite a while?",
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.021680355072021484,
      "inference_time": 9.275612354278564,
      "num_events": 5,
      "top_rrf_score": 0.09523809523809523,
      "mean_rrf_score": 0.09009609813090289,
      "visual_top_sim": 0.3081420660018921,
      "visual_mean_sim": 0.3184724450111389,
      "semantic_top_sim": 0.50489741563797,
      "semantic_mean_sim": 0.4696195721626282,
      "entity_top_sim": 0.35038262605667114,
      "entity_mean_sim": 0.3013625741004944
    },
    {
      "video_id": "484",
      "question_id": "484-1",
      "task_type": "Spatial Reasoning",
      "duration": "medium",
      "question": "What is the theme of the performance in the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.019345998764038086,
      "inference_time": 9.130142211914062,
      "num_events": 5,
      "top_rrf_score": 0.09696969696969696,
      "mean_rrf_score": 0.09191721018808305,
      "visual_top_sim": 0.2781608998775482,
      "visual_mean_sim": 0.2745305299758911,
      "semantic_top_sim": 0.5573508143424988,
      "semantic_mean_sim": 0.49832653999328613,
      "entity_top_sim": 0.4680791199207306,
      "entity_mean_sim": 0.41629356145858765
    },
    {
      "video_id": "484",
      "question_id": "484-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many seats are on stage in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.01926732063293457,
      "inference_time": 9.180294752120972,
      "num_events": 5,
      "top_rrf_score": 0.09488636363636363,
      "mean_rrf_score": 0.08963851786021684,
      "visual_top_sim": 0.2904224097728729,
      "visual_mean_sim": 0.28031960129737854,
      "semantic_top_sim": 0.48315736651420593,
      "semantic_mean_sim": 0.48599618673324585,
      "entity_top_sim": 0.48922479152679443,
      "entity_mean_sim": 0.438774973154068
    },
    {
      "video_id": "484",
      "question_id": "484-3",
      "task_type": "Temporal Perception",
      "duration": "medium",
      "question": "What can be inferred about the story at the beginning of the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.019445180892944336,
      "inference_time": 9.236738204956055,
      "num_events": 5,
      "top_rrf_score": 0.09393939393939393,
      "mean_rrf_score": 0.09090075099437928,
      "visual_top_sim": 0.2642056941986084,
      "visual_mean_sim": 0.2638971209526062,
      "semantic_top_sim": 0.42751365900039673,
      "semantic_mean_sim": 0.38448819518089294,
      "entity_top_sim": 0.21822959184646606,
      "entity_mean_sim": 0.21178026497364044
    },
    {
      "video_id": "477",
      "question_id": "477-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which team participates in the first five matches in this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.021077871322631836,
      "inference_time": 13.086882829666138,
      "num_events": 5,
      "top_rrf_score": 0.0919728595250417,
      "mean_rrf_score": 0.0853983566240604,
      "visual_top_sim": 0.31565994024276733,
      "visual_mean_sim": 0.3057918846607208,
      "semantic_top_sim": 0.4960889220237732,
      "semantic_mean_sim": 0.4725760817527771,
      "entity_top_sim": 0.46494796872138977,
      "entity_mean_sim": 0.45733729004859924
    },
    {
      "video_id": "477",
      "question_id": "477-2",
      "task_type": "OCR Problems",
      "duration": "medium",
      "question": "What is the number written on the back of Nishida?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.020722389221191406,
      "inference_time": 12.861921787261963,
      "num_events": 5,
      "top_rrf_score": 0.09684139784946236,
      "mean_rrf_score": 0.08921028251347964,
      "visual_top_sim": 0.2771144211292267,
      "visual_mean_sim": 0.26748526096343994,
      "semantic_top_sim": 0.2347681224346161,
      "semantic_mean_sim": 0.12036599963903427,
      "entity_top_sim": 0.23958465456962585,
      "entity_mean_sim": 0.2353627234697342
    },
    {
      "video_id": "477",
      "question_id": "477-3",
      "task_type": "Action Recognition",
      "duration": "medium",
      "question": "How does JPN team usually win a point in this video?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.020621538162231445,
      "inference_time": 12.835684061050415,
      "num_events": 5,
      "top_rrf_score": 0.09399509803921569,
      "mean_rrf_score": 0.08919561386892576,
      "visual_top_sim": 0.3244357109069824,
      "visual_mean_sim": 0.3222564160823822,
      "semantic_top_sim": 0.4766805171966553,
      "semantic_mean_sim": 0.47788819670677185,
      "entity_top_sim": 0.48444414138793945,
      "entity_mean_sim": 0.4461985230445862
    },
    {
      "video_id": "436",
      "question_id": "436-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What can be seen in the sky during the race?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.021062135696411133,
      "inference_time": 12.332127332687378,
      "num_events": 5,
      "top_rrf_score": 0.09022435897435896,
      "mean_rrf_score": 0.08801550626952952,
      "visual_top_sim": 0.2916250228881836,
      "visual_mean_sim": 0.2813020348548889,
      "semantic_top_sim": 0.5358462333679199,
      "semantic_mean_sim": 0.4982028603553772,
      "entity_top_sim": 0.4501936137676239,
      "entity_mean_sim": 0.5153822898864746
    },
    {
      "video_id": "436",
      "question_id": "436-2",
      "task_type": "Counting Problem",
      "duration": "medium",
      "question": "How many cars does player use?",
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.020910024642944336,
      "inference_time": 12.311285018920898,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.08886364242013492,
      "visual_top_sim": 0.2888201177120209,
      "visual_mean_sim": 0.28699588775634766,
      "semantic_top_sim": 0.5209652185440063,
      "semantic_mean_sim": 0.4155789017677307,
      "entity_top_sim": 0.49660179018974304,
      "entity_mean_sim": 0.41885361075401306
    },
    {
      "video_id": "436",
      "question_id": "436-3",
      "task_type": "Spatial Perception",
      "duration": "medium",
      "question": "What is the weather like?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.024859189987182617,
      "inference_time": 12.584059000015259,
      "num_events": 5,
      "top_rrf_score": 0.09230769230769231,
      "mean_rrf_score": 0.08917817833140414,
      "visual_top_sim": 0.22929009795188904,
      "visual_mean_sim": 0.23648309707641602,
      "semantic_top_sim": 0.17311900854110718,
      "semantic_mean_sim": 0.13979732990264893,
      "entity_top_sim": 0.32393962144851685,
      "entity_mean_sim": 0.30520662665367126
    },
    {
      "video_id": "323",
      "question_id": "323-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "Which cellular structure is responsible for receiving proteins according to the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.025951623916625977,
      "inference_time": 10.47995924949646,
      "num_events": 5,
      "top_rrf_score": 0.08940092165898617,
      "mean_rrf_score": 0.08648588282951111,
      "visual_top_sim": 0.3079388439655304,
      "visual_mean_sim": 0.31746283173561096,
      "semantic_top_sim": 0.5954794883728027,
      "semantic_mean_sim": 0.5710627436637878,
      "entity_top_sim": 0.5969089269638062,
      "entity_mean_sim": 0.5131407380104065
    },
    {
      "video_id": "323",
      "question_id": "323-2",
      "task_type": "Attribute Perception",
      "duration": "medium",
      "question": "Which components are part of the object described in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.021247148513793945,
      "inference_time": 10.457812786102295,
      "num_events": 5,
      "top_rrf_score": 0.08863636363636364,
      "mean_rrf_score": 0.0844199141311767,
      "visual_top_sim": 0.29298609495162964,
      "visual_mean_sim": 0.2816482484340668,
      "semantic_top_sim": 0.42376434803009033,
      "semantic_mean_sim": 0.4317205548286438,
      "entity_top_sim": 0.41808319091796875,
      "entity_mean_sim": 0.42260462045669556
    },
    {
      "video_id": "323",
      "question_id": "323-3",
      "task_type": "Temporal Reasoning",
      "duration": "medium",
      "question": "What is the correct chronological order in which the following parts of the video appear?\n(a) Human lungs.\n(b) Protein folding distortion.\n(c) Mice, plants and cells.",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.02123260498046875,
      "inference_time": 10.451067924499512,
      "num_events": 5,
      "top_rrf_score": 0.0931547619047619,
      "mean_rrf_score": 0.08660085883921528,
      "visual_top_sim": 0.30194810032844543,
      "visual_mean_sim": 0.298088014125824,
      "semantic_top_sim": 0.5095083117485046,
      "semantic_mean_sim": 0.4413737654685974,
      "entity_top_sim": 0.3497559726238251,
      "entity_mean_sim": 0.34320345520973206
    },
    {
      "video_id": "536",
      "question_id": "536-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What is the first food the main character in the video tried?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.02195286750793457,
      "inference_time": 11.319061994552612,
      "num_events": 5,
      "top_rrf_score": 0.09291982922201139,
      "mean_rrf_score": 0.09031270834401764,
      "visual_top_sim": 0.3249693214893341,
      "visual_mean_sim": 0.3192784786224365,
      "semantic_top_sim": 0.5316739678382874,
      "semantic_mean_sim": 0.5259441137313843,
      "entity_top_sim": 0.37175554037094116,
      "entity_mean_sim": 0.42043638229370117
    },
    {
      "video_id": "536",
      "question_id": "536-2",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "Of the following foods, what does the protagonist in the video prefer?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.021455049514770508,
      "inference_time": 11.502154111862183,
      "num_events": 5,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.0916418755555378,
      "visual_top_sim": 0.3076746463775635,
      "visual_mean_sim": 0.3009307384490967,
      "semantic_top_sim": 0.5493924617767334,
      "semantic_mean_sim": 0.5268133878707886,
      "entity_top_sim": 0.42023342847824097,
      "entity_mean_sim": 0.4555133283138275
    },
    {
      "video_id": "536",
      "question_id": "536-3",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "The ad in the video is inserted while the main character is eating what?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.02170085906982422,
      "inference_time": 11.330351829528809,
      "num_events": 5,
      "top_rrf_score": 0.0920794930875576,
      "mean_rrf_score": 0.08846167767644848,
      "visual_top_sim": 0.3104705512523651,
      "visual_mean_sim": 0.3108598291873932,
      "semantic_top_sim": 0.5069950819015503,
      "semantic_mean_sim": 0.4881196916103363,
      "entity_top_sim": 0.3664079010486603,
      "entity_mean_sim": 0.35241037607192993
    },
    {
      "video_id": "575",
      "question_id": "575-1",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What are the animals featured in the video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.019077062606811523,
      "inference_time": 9.006468772888184,
      "num_events": 3,
      "top_rrf_score": 0.09791666666666665,
      "mean_rrf_score": 0.09684139784946237,
      "visual_top_sim": 0.2872896194458008,
      "visual_mean_sim": 0.29653117060661316,
      "semantic_top_sim": 0.5351155400276184,
      "semantic_mean_sim": 0.48902904987335205,
      "entity_top_sim": 0.4133344292640686,
      "entity_mean_sim": 0.3887386620044708
    },
    {
      "video_id": "575",
      "question_id": "575-2",
      "task_type": "Action Reasoning",
      "duration": "medium",
      "question": "What color is the house?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.018002748489379883,
      "inference_time": 9.021314144134521,
      "num_events": 3,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.09684139784946237,
      "visual_top_sim": 0.22474029660224915,
      "visual_mean_sim": 0.22085674107074738,
      "semantic_top_sim": 0.280253529548645,
      "semantic_mean_sim": 0.2209409475326538,
      "entity_top_sim": 0.47511354088783264,
      "entity_mean_sim": 0.39128968119621277
    },
    {
      "video_id": "575",
      "question_id": "575-3",
      "task_type": "Object Recognition",
      "duration": "medium",
      "question": "What is between the yellow fox and the white fox?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019086837768554688,
      "inference_time": 9.040190696716309,
      "num_events": 3,
      "top_rrf_score": 0.09791666666666667,
      "mean_rrf_score": 0.09684139784946237,
      "visual_top_sim": 0.2738642990589142,
      "visual_mean_sim": 0.26435747742652893,
      "semantic_top_sim": 0.6165319085121155,
      "semantic_mean_sim": 0.5505216717720032,
      "entity_top_sim": 0.400327205657959,
      "entity_mean_sim": 0.4038611352443695
    },
    {
      "video_id": "664",
      "question_id": "664-1",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is the video mainly about?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.02383255958557129,
      "inference_time": 24.047889709472656,
      "num_events": 5,
      "top_rrf_score": 0.078561736770692,
      "mean_rrf_score": 0.07648223396938877,
      "visual_top_sim": 0.23118911683559418,
      "visual_mean_sim": 0.24394986033439636,
      "semantic_top_sim": 0.4588810205459595,
      "semantic_mean_sim": 0.43994730710983276,
      "entity_top_sim": 0.3233032524585724,
      "entity_mean_sim": 0.27501532435417175
    },
    {
      "video_id": "664",
      "question_id": "664-2",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "Who is the biological father of the girl in the second case in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.02312779426574707,
      "inference_time": 23.6981041431427,
      "num_events": 5,
      "top_rrf_score": 0.09190718732314657,
      "mean_rrf_score": 0.08334215620986256,
      "visual_top_sim": 0.31187373399734497,
      "visual_mean_sim": 0.2951980531215668,
      "semantic_top_sim": 0.6206479072570801,
      "semantic_mean_sim": 0.6112385392189026,
      "entity_top_sim": 0.4456334710121155,
      "entity_mean_sim": 0.4111329913139343
    },
    {
      "video_id": "664",
      "question_id": "664-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the view held by the female witness present in the first case in the video?",
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.022981882095336914,
      "inference_time": 23.653523445129395,
      "num_events": 5,
      "top_rrf_score": 0.08149350649350649,
      "mean_rrf_score": 0.07889462453072275,
      "visual_top_sim": 0.2976531386375427,
      "visual_mean_sim": 0.3056655526161194,
      "semantic_top_sim": 0.6781011819839478,
      "semantic_mean_sim": 0.668857753276825,
      "entity_top_sim": 0.516990065574646,
      "entity_mean_sim": 0.4531879425048828
    },
    {
      "video_id": "794",
      "question_id": "794-1",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is the primary focus of this video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.028165578842163086,
      "inference_time": 29.68096351623535,
      "num_events": 5,
      "top_rrf_score": 0.0783733527366804,
      "mean_rrf_score": 0.06927948956868028,
      "visual_top_sim": 0.2955947816371918,
      "visual_mean_sim": 0.26998090744018555,
      "semantic_top_sim": 0.4814690351486206,
      "semantic_mean_sim": 0.4743768274784088,
      "entity_top_sim": 0.3692542612552643,
      "entity_mean_sim": 0.3841726779937744
    },
    {
      "video_id": "794",
      "question_id": "794-2",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "What is the key to the illusion of bending a teaspoon with his mind?",
      "predicted": "B",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.03062725067138672,
      "inference_time": 29.734150171279907,
      "num_events": 5,
      "top_rrf_score": 0.0928030303030303,
      "mean_rrf_score": 0.08164139628049802,
      "visual_top_sim": 0.2648189961910248,
      "visual_mean_sim": 0.2549871802330017,
      "semantic_top_sim": 0.3043682873249054,
      "semantic_mean_sim": 0.29967474937438965,
      "entity_top_sim": 0.26298460364341736,
      "entity_mean_sim": 0.2448371946811676
    },
    {
      "video_id": "794",
      "question_id": "794-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the purpose of the mirrors in the last magic?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.027758121490478516,
      "inference_time": 29.845181703567505,
      "num_events": 5,
      "top_rrf_score": 0.09083191850594227,
      "mean_rrf_score": 0.08566986359342925,
      "visual_top_sim": 0.2970936596393585,
      "visual_mean_sim": 0.29292717576026917,
      "semantic_top_sim": 0.3173513412475586,
      "semantic_mean_sim": 0.3030003607273102,
      "entity_top_sim": 0.24388490617275238,
      "entity_mean_sim": 0.2513202130794525
    },
    {
      "video_id": "641",
      "question_id": "641-1",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What future human activity does the video discuss?",
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.040667057037353516,
      "inference_time": 31.083350658416748,
      "num_events": 5,
      "top_rrf_score": 0.08044583044583045,
      "mean_rrf_score": 0.07536877129360744,
      "visual_top_sim": 0.2722497284412384,
      "visual_mean_sim": 0.26779255270957947,
      "semantic_top_sim": 0.4661838114261627,
      "semantic_mean_sim": 0.4739072918891907,
      "entity_top_sim": 0.329364150762558,
      "entity_mean_sim": 0.34176144003868103
    },
    {
      "video_id": "641",
      "question_id": "641-2",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "In what order are the following planets introduced in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.07104635238647461,
      "inference_time": 31.225087642669678,
      "num_events": 5,
      "top_rrf_score": 0.09161036036036035,
      "mean_rrf_score": 0.08124415758778587,
      "visual_top_sim": 0.3143813908100128,
      "visual_mean_sim": 0.31775030493736267,
      "semantic_top_sim": 0.5804169178009033,
      "semantic_mean_sim": 0.5269112586975098,
      "entity_top_sim": 0.48601847887039185,
      "entity_mean_sim": 0.46723365783691406
    },
    {
      "video_id": "641",
      "question_id": "641-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What are the planets that Dr.David Grinspoon and Dr. Heidi B. Hammel research?",
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.03859567642211914,
      "inference_time": 31.089131832122803,
      "num_events": 5,
      "top_rrf_score": 0.09308755760368663,
      "mean_rrf_score": 0.07789627798990627,
      "visual_top_sim": 0.2951594591140747,
      "visual_mean_sim": 0.291615754365921,
      "semantic_top_sim": 0.38431841135025024,
      "semantic_mean_sim": 0.3757691979408264,
      "entity_top_sim": 0.45982569456100464,
      "entity_mean_sim": 0.4113982617855072
    },
    {
      "video_id": "883",
      "question_id": "883-1",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "Which move in the video was done in both the warm-up phase and the official workout phase of Monday's workout?",
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "retrieval_time": 0.022052526473999023,
      "inference_time": 30.096433639526367,
      "num_events": 5,
      "top_rrf_score": 0.09589442815249266,
      "mean_rrf_score": 0.08982126136650256,
      "visual_top_sim": 0.2740536034107208,
      "visual_mean_sim": 0.26956886053085327,
      "semantic_top_sim": 0.6420904397964478,
      "semantic_mean_sim": 0.6160750985145569,
      "entity_top_sim": 0.4690206050872803,
      "entity_mean_sim": 0.46848487854003906
    },
    {
      "video_id": "883",
      "question_id": "883-2",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the difference in appearance between the female lead in the video, Friday and Wednesday?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.021450042724609375,
      "inference_time": 29.860095977783203,
      "num_events": 5,
      "top_rrf_score": 0.08377525252525253,
      "mean_rrf_score": 0.08166009762987965,
      "visual_top_sim": 0.26457536220550537,
      "visual_mean_sim": 0.2527446746826172,
      "semantic_top_sim": 0.4273970127105713,
      "semantic_mean_sim": 0.44188207387924194,
      "entity_top_sim": 0.1889854222536087,
      "entity_mean_sim": 0.2111472189426422
    },
    {
      "video_id": "883",
      "question_id": "883-3",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "Which of the following describes the heroine's weekly fitness programme correctly?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.02127814292907715,
      "inference_time": 29.94157099723816,
      "num_events": 5,
      "top_rrf_score": 0.09190718732314657,
      "mean_rrf_score": 0.08865691289254377,
      "visual_top_sim": 0.2742987871170044,
      "visual_mean_sim": 0.25795620679855347,
      "semantic_top_sim": 0.3738216757774353,
      "semantic_mean_sim": 0.4042610228061676,
      "entity_top_sim": 0.3788706362247467,
      "entity_mean_sim": 0.3799836039543152
    },
    {
      "video_id": "751",
      "question_id": "751-1",
      "task_type": "Counting Problem",
      "duration": "long",
      "question": "How many goals did the number 9 of the blue team score in the match?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.023449420928955078,
      "inference_time": 26.141030073165894,
      "num_events": 5,
      "top_rrf_score": 0.08869685624903842,
      "mean_rrf_score": 0.084051515778271,
      "visual_top_sim": 0.30229970812797546,
      "visual_mean_sim": 0.2890010178089142,
      "semantic_top_sim": 0.40958917140960693,
      "semantic_mean_sim": 0.43011799454689026,
      "entity_top_sim": 0.5150625705718994,
      "entity_mean_sim": 0.518365204334259
    },
    {
      "video_id": "751",
      "question_id": "751-2",
      "task_type": "Counting Problem",
      "duration": "long",
      "question": "How many timeout substitutions were there during the game in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.02484750747680664,
      "inference_time": 26.29618811607361,
      "num_events": 5,
      "top_rrf_score": 0.09059139784946235,
      "mean_rrf_score": 0.07449451027163785,
      "visual_top_sim": 0.29140302538871765,
      "visual_mean_sim": 0.2749021649360657,
      "semantic_top_sim": 0.3793787360191345,
      "semantic_mean_sim": 0.33760231733322144,
      "entity_top_sim": 0.36057141423225403,
      "entity_mean_sim": 0.33206725120544434
    },
    {
      "video_id": "751",
      "question_id": "751-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "How did the match in the video progress in terms of scoring, leading up to its conclusion?",
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.024500131607055664,
      "inference_time": 26.231027364730835,
      "num_events": 5,
      "top_rrf_score": 0.08860727086533537,
      "mean_rrf_score": 0.07650226366204317,
      "visual_top_sim": 0.303854376077652,
      "visual_mean_sim": 0.2952212393283844,
      "semantic_top_sim": 0.5604923963546753,
      "semantic_mean_sim": 0.5528939366340637,
      "entity_top_sim": 0.4101346731185913,
      "entity_mean_sim": 0.38446810841560364
    },
    {
      "video_id": "786",
      "question_id": "786-1",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "How does Snow White come to meet the seven miners?",
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.022336483001708984,
      "inference_time": 24.793666124343872,
      "num_events": 5,
      "top_rrf_score": 0.08439710194928411,
      "mean_rrf_score": 0.08166465096121632,
      "visual_top_sim": 0.28505316376686096,
      "visual_mean_sim": 0.2902502417564392,
      "semantic_top_sim": 0.2065776288509369,
      "semantic_mean_sim": 0.19640082120895386,
      "entity_top_sim": 0.33068788051605225,
      "entity_mean_sim": 0.3196175694465637
    },
    {
      "video_id": "786",
      "question_id": "786-2",
      "task_type": "Action Recognition",
      "duration": "long",
      "question": "What is the Queen's fate in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.024086952209472656,
      "inference_time": 24.787943840026855,
      "num_events": 5,
      "top_rrf_score": 0.08731085486303702,
      "mean_rrf_score": 0.0839514744649256,
      "visual_top_sim": 0.2760942280292511,
      "visual_mean_sim": 0.27729281783103943,
      "semantic_top_sim": 0.41587355732917786,
      "semantic_mean_sim": 0.391665518283844,
      "entity_top_sim": 0.32694393396377563,
      "entity_mean_sim": 0.32288658618927
    },
    {
      "video_id": "786",
      "question_id": "786-3",
      "task_type": "Action Reasoning",
      "duration": "long",
      "question": "What inference can be made about the actress on her interactions in the video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.02294921875,
      "inference_time": 24.811057806015015,
      "num_events": 5,
      "top_rrf_score": 0.09161036036036035,
      "mean_rrf_score": 0.08463482210985455,
      "visual_top_sim": 0.2601623833179474,
      "visual_mean_sim": 0.266148179769516,
      "semantic_top_sim": 0.46676596999168396,
      "semantic_mean_sim": 0.4491359293460846,
      "entity_top_sim": 0.2673253118991852,
      "entity_mean_sim": 0.22550955414772034
    },
    {
      "video_id": "896",
      "question_id": "896-1",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What sets apart the third set of stickers from the other two?",
      "predicted": "B",
      "correct": "C",
      "is_correct": false,
      "retrieval_time": 0.02147507667541504,
      "inference_time": 26.723373889923096,
      "num_events": 5,
      "top_rrf_score": 0.09488636363636363,
      "mean_rrf_score": 0.09104113286039284,
      "visual_top_sim": 0.26472869515419006,
      "visual_mean_sim": 0.265847384929657,
      "semantic_top_sim": 0.5524969100952148,
      "semantic_mean_sim": 0.48397573828697205,
      "entity_top_sim": 0.5478121042251587,
      "entity_mean_sim": 0.5328866839408875
    },
    {
      "video_id": "896",
      "question_id": "896-2",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "What is the proper sequence for the following items used in this video?\n(a) Stickers.\n(b) Watercolor pencils.\n(c) Gems.\n(d) Glue paper.",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.02319788932800293,
      "inference_time": 26.552833557128906,
      "num_events": 5,
      "top_rrf_score": 0.08840579710144927,
      "mean_rrf_score": 0.08143849140753052,
      "visual_top_sim": 0.29625368118286133,
      "visual_mean_sim": 0.30143648386001587,
      "semantic_top_sim": 0.5469497442245483,
      "semantic_mean_sim": 0.48965567350387573,
      "entity_top_sim": 0.5008141398429871,
      "entity_mean_sim": 0.4710284173488617
    },
    {
      "video_id": "896",
      "question_id": "896-3",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is captured in this video?",
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.021410226821899414,
      "inference_time": 26.520453929901123,
      "num_events": 5,
      "top_rrf_score": 0.08521973698999663,
      "mean_rrf_score": 0.08022948709183172,
      "visual_top_sim": 0.28278613090515137,
      "visual_mean_sim": 0.2765659689903259,
      "semantic_top_sim": 0.4405955374240875,
      "semantic_mean_sim": 0.40966305136680603,
      "entity_top_sim": 0.30070340633392334,
      "entity_mean_sim": 0.3172946870326996
    },
    {
      "video_id": "699",
      "question_id": "699-1",
      "task_type": "Action Recognition",
      "duration": "long",
      "question": "In line with the video evidence, what does the team do in the first story?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.041283607482910156,
      "inference_time": 20.776304006576538,
      "num_events": 5,
      "top_rrf_score": 0.0804373362248125,
      "mean_rrf_score": 0.07307379277666008,
      "visual_top_sim": 0.2744631767272949,
      "visual_mean_sim": 0.263307124376297,
      "semantic_top_sim": 0.3599672019481659,
      "semantic_mean_sim": 0.36122551560401917,
      "entity_top_sim": 0.24463099241256714,
      "entity_mean_sim": 0.2621028423309326
    },
    {
      "video_id": "699",
      "question_id": "699-2",
      "task_type": "Object Recognition",
      "duration": "long",
      "question": "In accordance with the video footage, who protects the mermaid?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.04082489013671875,
      "inference_time": 22.05827784538269,
      "num_events": 5,
      "top_rrf_score": 0.08789830841856805,
      "mean_rrf_score": 0.08416956630842298,
      "visual_top_sim": 0.30053868889808655,
      "visual_mean_sim": 0.299277126789093,
      "semantic_top_sim": 0.6163187623023987,
      "semantic_mean_sim": 0.6399160623550415,
      "entity_top_sim": 0.4507056772708893,
      "entity_mean_sim": 0.41963300108909607
    },
    {
      "video_id": "699",
      "question_id": "699-3",
      "task_type": "Action Recognition",
      "duration": "long",
      "question": "What happens when the figures in the video are electrocuted?",
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "retrieval_time": 0.04024553298950195,
      "inference_time": 20.76747989654541,
      "num_events": 5,
      "top_rrf_score": 0.08665223665223665,
      "mean_rrf_score": 0.08002670326355805,
      "visual_top_sim": 0.2753986716270447,
      "visual_mean_sim": 0.2729237377643585,
      "semantic_top_sim": 0.4205244481563568,
      "semantic_mean_sim": 0.4253905713558197,
      "entity_top_sim": 0.30750638246536255,
      "entity_mean_sim": 0.310837060213089
    },
    {
      "video_id": "636",
      "question_id": "636-1",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "Which of the following best summarizes the main concern of those who advocate for a reduction in the balance sheet in the video?",
      "predicted": "B",
      "correct": "B",
      "is_correct": true,
      "retrieval_time": 0.037175655364990234,
      "inference_time": 27.427698850631714,
      "num_events": 5,
      "top_rrf_score": 0.09500316255534472,
      "mean_rrf_score": 0.08157217911953193,
      "visual_top_sim": 0.305706262588501,
      "visual_mean_sim": 0.2933656573295593,
      "semantic_top_sim": 0.488384485244751,
      "semantic_mean_sim": 0.46162548661231995,
      "entity_top_sim": 0.4014843702316284,
      "entity_mean_sim": 0.3647993206977844
    },
    {
      "video_id": "636",
      "question_id": "636-2",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "According to the video, what is one reason why the relationship between inflation and wage inflation has weakened in recent decades?",
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.037361860275268555,
      "inference_time": 27.36254334449768,
      "num_events": 5,
      "top_rrf_score": 0.09190718732314657,
      "mean_rrf_score": 0.08384579622520646,
      "visual_top_sim": 0.3395376205444336,
      "visual_mean_sim": 0.3455088436603546,
      "semantic_top_sim": 0.5437268614768982,
      "semantic_mean_sim": 0.48574790358543396,
      "entity_top_sim": 0.33964914083480835,
      "entity_mean_sim": 0.2872388958930969
    },
    {
      "video_id": "636",
      "question_id": "636-3",
      "task_type": "Information Synopsis",
      "duration": "long",
      "question": "What is the main focus of this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.03643012046813965,
      "inference_time": 27.567075967788696,
      "num_events": 5,
      "top_rrf_score": 0.07767857142857143,
      "mean_rrf_score": 0.0694216146960698,
      "visual_top_sim": 0.29696395993232727,
      "visual_mean_sim": 0.2699771821498871,
      "semantic_top_sim": 0.5065252780914307,
      "semantic_mean_sim": 0.5087844729423523,
      "entity_top_sim": 0.3437674641609192,
      "entity_mean_sim": 0.38359397649765015
    },
    {
      "video_id": "624",
      "question_id": "624-1",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What is the speaker's opinion on reversing type 2 diabetes?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.02157878875732422,
      "inference_time": 24.2716543674469,
      "num_events": 5,
      "top_rrf_score": 0.08789830841856805,
      "mean_rrf_score": 0.08612157679488869,
      "visual_top_sim": 0.3324185013771057,
      "visual_mean_sim": 0.3063414990901947,
      "semantic_top_sim": 0.4119609296321869,
      "semantic_mean_sim": 0.3713516592979431,
      "entity_top_sim": 0.2086920291185379,
      "entity_mean_sim": 0.27701878547668457
    },
    {
      "video_id": "624",
      "question_id": "624-2",
      "task_type": "Temporal Reasoning",
      "duration": "long",
      "question": "What is the order in which the following is introduced in the video?\n\u2460 Insulin Resistance Explained.\n\u2461 Case studies.\n\u2462 How to Reverse Type 2 Diabetes.",
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "retrieval_time": 0.022139549255371094,
      "inference_time": 24.31239604949951,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.08631670282642454,
      "visual_top_sim": 0.332535982131958,
      "visual_mean_sim": 0.3017770051956177,
      "semantic_top_sim": 0.49268078804016113,
      "semantic_mean_sim": 0.44668346643447876,
      "entity_top_sim": 0.41902685165405273,
      "entity_mean_sim": 0.3423634171485901
    },
    {
      "video_id": "624",
      "question_id": "624-3",
      "task_type": "Object Reasoning",
      "duration": "long",
      "question": "What treatment is universally implemented in all the case studies featured in the video?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.022703886032104492,
      "inference_time": 24.125914573669434,
      "num_events": 5,
      "top_rrf_score": 0.09444444444444444,
      "mean_rrf_score": 0.08825566131132614,
      "visual_top_sim": 0.28306177258491516,
      "visual_mean_sim": 0.29195159673690796,
      "semantic_top_sim": 0.4548061788082123,
      "semantic_mean_sim": 0.4124695360660553,
      "entity_top_sim": 0.32009294629096985,
      "entity_mean_sim": 0.295382559299469
    },
    {
      "video_id": "001",
      "question_id": "001-1",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?",
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "retrieval_time": 0.02374720573425293,
      "inference_time": 7.263950824737549,
      "num_events": 5,
      "top_rrf_score": 0.09892473118279568,
      "mean_rrf_score": 0.093933715715025,
      "visual_top_sim": 0.3205046057701111,
      "visual_mean_sim": 0.2974861264228821,
      "semantic_top_sim": 0.5385385751724243,
      "semantic_mean_sim": 0.4680706858634949,
      "entity_top_sim": 0.47422003746032715,
      "entity_mean_sim": 0.432415634393692
    },
    {
      "video_id": "001",
      "question_id": "001-2",
      "task_type": "Information Synopsis",
      "duration": "short",
      "question": "What is the genre of this video?",
      "predicted": "A",
      "correct": "A",
      "is_correct": true,
      "retrieval_time": 0.019142627716064453,
      "inference_time": 7.111289739608765,
      "num_events": 5,
      "top_rrf_score": 0.09784946236559139,
      "mean_rrf_score": 0.09325126091492315,
      "visual_top_sim": 0.2553223669528961,
      "visual_mean_sim": 0.23893412947654724,
      "semantic_top_sim": 0.45221877098083496,
      "semantic_mean_sim": 0.3812950551509857,
      "entity_top_sim": 0.35593605041503906,
      "entity_mean_sim": 0.3356921374797821
    },
    {
      "video_id": "001",
      "question_id": "001-3",
      "task_type": "Counting Problem",
      "duration": "short",
      "question": "How many red socks are above the fireplace at the end of this video?",
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "retrieval_time": 0.01959824562072754,
      "inference_time": 7.105634927749634,
      "num_events": 5,
      "top_rrf_score": 0.0989247311827957,
      "mean_rrf_score": 0.09339800142931073,
      "visual_top_sim": 0.3388385474681854,
      "visual_mean_sim": 0.2716771066188812,
      "semantic_top_sim": 0.46404600143432617,
      "semantic_mean_sim": 0.3971681594848633,
      "entity_top_sim": 0.38642236590385437,
      "entity_mean_sim": 0.3794323801994324
    }
  ]
}